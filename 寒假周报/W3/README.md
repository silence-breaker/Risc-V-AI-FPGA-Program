# 第三周任务完成情况（2.3.-2.12）

## 整体完成情况

1. 三人协作完成板卡编译工具icraft文档的本地化，搭建了知识库，目前WJ和LCJ完成了自己负责的部分，WYC完成了一半。
2. 确定了集创赛的具体项目方案：

```
高尚公益风 ——《基于端侧大模型的视障辅助导盲伴侣》

核心概念： 视障人士看不见世界，你的系统充当他们的“眼睛”和“嘴巴”。摄像头看到东西，通过 NPU 识别，再通过 FPGA 生成准确的语音描述（或大号文字）。

为什么“实际”？ 盲人导盲需要极低的延迟（不能撞墙）和隐私保护（不想传到云端），这完美契合你的 FPGA 端侧加速（低延迟） 和 1.58-bit（离线运行） 的特点。

演示场景：

场景： 找一个同学扮演“盲人”（戴墨镜），手持开发板（或把摄像头挂胸前）。

动作： 他走到障碍物前（比如一堆乱放的椅子）。

识别 (NPU)： 快速识别出 Chair, Obstacle。

决策 (FPGA)： 我的 1.58-bit 模型接收这些词，生成警示语："Warning: Multiple chairs ahead. Please move to the left." （警告：前方有多把椅子，请向左绕行。）

输出： 屏幕显示大大的红色警告文字（甚至可以接个简单的蜂鸣器或耳机播放）。

技术实现路径（纯软件+FPGA）：

NPU： 跑 YOLO，只负责输出物体名称和坐标。

CPU： 计算障碍物距离（根据框的大小），如果太近，触发 FPGA。

FPGA (TDPU)： 输入 [Obstacle: Chair, Distance: Close]，输出自然语言建议。这里展示了 LLM 的逻辑判断能力（它知道椅子是挡路的，而不是用来坐的）。
```

-  针对集创赛方案，WJ探索自定义硬算子在icraft工具链下的开发与仿真流程，同时进一步完善了TDPU核心模块设计
-  针对集创赛方案，LCJ完成了NPU端的YOLO模型处理视频流仿真，验证了在icraft工具链下的开发流程

## 详细报告链接

[TDPU核心模块设计与icraft工具链适配报告](https://github.com/silence-breaker/TDPU)

[NPU端视频流处理仿真报告](https://github.com/silence-breaker/Risc-V-AI-FPGA-Program/blob/LCJ/w3/FMQL30TAI%20YOLOv5%20%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E4%BB%BF%E7%9C%9F%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93%E6%8A%A5%E5%91%8A/FMQL30TAI%20YOLOv5%20%E8%A7%86%E9%A2%91%E6%B5%81%E6%8E%A8%E7%90%86%E4%BB%BF%E7%9C%9F%E5%B7%A5%E4%BD%9C%E6%80%BB%E7%BB%93%E6%8A%A5%E5%91%8A.md)

## 之后安排

1. 2.13-2.20号WJ和LCJ休息，WYC回国后继续完成icraft文档本地化的剩余部分
2. 2.22号三人继续准备集创赛（开工）
