# 大学生创新训练项目申报书：基于 RISC-V 的 Transformer 模型 1.58-bit 矩阵加速架构研究

FPGA 板子选型：正点原子 ZYNQ Xilinx 7020 开发板（1288）

# 1. 研究目的

**（1）背景与挑战：**  
当前，以 Transformer 架构为核心的大语言模型（LLM）已成为人工智能领域的绝对主流。然而，随着模型参数量呈指数级增长，传统计算架构面临着严峻的“**内存墙（Memory Wall）**”与“**功耗墙（Power Wall）**”挑战。在边缘端设备（如嵌入式系统、移动终端）上部署大模型时，高昂的内存带宽需求和巨大的乘法运算能耗成为主要瓶颈，限制了 AI 技术的普惠与落地。

**（2）理论依据与机遇：**  
2024 年，微软研究院提出的 **BitNet b 1.58** 理论证明了将模型权重极致量化为 {−1,0,1}三值（1.58-bit）后，依然能保持与全精度模型相当的性能。这一突破为硬件设计带来了革命性机遇：它使得深度学习计算可以彻底摆脱昂贵的浮点乘法器，转而使用极低成本的简单加减法逻辑。

**（3）本项目研究目的：**  
本项目旨在探索一种**面向 Transformer 的极致高效计算范式**。我们将基于 RISC-V 开源指令集架构，设计并实现一款在 **FPGA 逻辑端集成** **RISC-V 处理器**与**1.58-bit 三值矩阵加速单元（TDPU）** 的异构 SoC（片上系统）。  
核心目的包括：

1. **验证“无乘法器”技术路线的可行性**：在 FPGA 硬件上实测 1.58-bit 量化在矩阵运算中的能效优势与资源占用情况。
2. **探索软硬协同的边缘端加速方案**：通过 RISC-V 软核与硬件加速器的异构协作，解决大模型算子在资源受限 FPGA 上的部署难题。
3. **构建全栈式原型系统**：从底层 RTL 硬件设计到上层 C 语言驱动，打通端侧 AI 推理的全流程，为未来低成本、低功耗的 AI 芯片设计提供架构参考与技术积累。

# 2. 国、内外研究现状和发展动态 (Research Status & Trends)

**（1）端侧大模型部署现状** 目前，边缘端 LLM 推理主要依赖 NVIDIA Jetson 或高通 NPU，且多采用 Int8/Int4 量化。虽然已有 Llama.cpp 等软件框架，但缺乏针对 **1.58-bit 极致量化** 的专用硬件 IP 核支持，导致 FPGA 在此领域的潜力未被充分挖掘。

**（2）低比特量化技术的新突破（发展动态）**  
2024 年 2 月，微软研究院发布论文，提出了 BitNet b 1.58 架构。该研究证明了三值权重{−1,0,1}在保持模型精度的同时，能将延迟降低数倍，能耗降低一个数量级。这一发现被认为是“后 Transformer 时代”硬件设计的重要转折点，引发了学术界对**无乘法器深度学习（Multiplier-free DL）** 的广泛关注。

**（3）RISC-V 与 AI 加速器的结合（现有差距）**  
在开源硬件领域，基于 RISC-V 的 AI 加速器研究多集中于传统的 CNN（卷积神经网络）图像处理，针对 Transformer 架构特有的 **GEMV（矩阵-向量乘）** 及 **Attention（注意力机制）** 的优化相对较少。  
特别是针对 **1.58-bit 极致量化**的专用硬件架构研究相对其他模型较少，现有的开源项目大多仍基于 Int 8 量化，未能充分利用三值权重带来的硬件简化优势。本项目拟通过对这方面知识的学习，深入探索 RISC-V 与 1.58-bit 加速器结合的全新架构。

**（4）生成式推理加速的难点** 与传统的图像分类不同，对话模型的推理分为 **Prefill（预填充）** 和 **Decode（解码）** 两个阶段。Decode 阶段是典型的“访存密集型”任务（Memory-bound），对内存带宽和 KV Cache 的读写效率要求极高。现有的开源 RISC-V 加速器大多未针对 KV Cache 进行优化，难以支撑流畅的对话体验。

# 3. 研究内容

本项目将构建一个针对 Transformer 核心算子优化的异构计算系统，主要研究内容分为控制层、计算层与互联层三个维度：

**（1）控制层：面向生成的 RISC-V 软核设计**

- **指令集架构**：设计支持 RV32I 的五级流水线 CPU。
- **KV Cache 管理器**：在软件驱动层实现 **KV Cache (Key-Value Cache)** 的动态显存分配与页面管理机制。RISC-V 负责将每一步推理产生的中间状态写入 DDR，并在下一步计算时通过 DMA 预取回片内，以支持长达 512~1024 token 的连续对话。
- **自定义指令**：增加 `GEMM_START` 等专用指令，通过 AXI-Lite 接口低延迟触发加速器。

**（2）计算层：1.58-bit 矩阵分块加速器（TDPU）设计**

- **无乘法器阵列**：利用 FPGA 的 LUT 资源将 8-bit $\times$ 1.58-bit 的乘法操作转化为高效的 **“选择-累加”** 逻辑树，彻底移除昂贵的 DSP 乘法器消耗。
- **并行解包器 (Unpacker)**：针对 2-bit 压缩权重，设计硬件并行的解包流水线，在一个时钟周期内将 64-bit 总线数据展开为 32 路控制信号，消除解码延迟。
- **流式接口封装**：将加速器封装为标准的 **AXI4-Stream** 接口 (Slave 用于接收权重/数据，Master 用于发送结果)，以适配 Xilinx 的 IP 生态。

 **(3) 存储与互联层：基于 Zynq 的异构存储架构**
利用 Zynq-7020 特有的架构优势，解决大模型权重的存储问题。

- **基于 AXI HP 的存储共享**：启用 Zynq 芯片特有的 **AXI HP** 接口，建立一条连接 PL 端（FPGA 逻辑）与 PS 端（DDR 控制器）的高速物理通道，使加速器能以 **DMA (直接存储器访问)** 方式直接读写板载 1GB DDR3 内存，无需自行实现复杂的 MIG 时序控制。
- **流式数据搬运引擎**：集成 Xilinx **AXI DMA** IP 核作为数据传输的主控引擎。它负责通过 AXI HP 接口从 DDR 拉取权重与数据，并将其转换为 **AXI-Stream** 数据流源源不断地输送给 TDPU，实现“存储-计算”之间的高效解耦与流水线传输。

**（4）软件与算法层：混合精度量化策略与驱动适配**

- **1.58-bit 量化感知训练 (QAT) 策略** 为确保三值化后的模型仍具备高精度的对话能力，本项目将基于 **BitNet b1.58** 论文，在 PyTorch 框架中实现完整的 **Quantization-Aware Training (QAT)** 流程：
 	- **1.58-bit 权重生成算法 (Forward Pass)**： 在前向传播过程中，实时将浮点权重 $W$ 映射为三值权重 $\tilde{W}$。算法步骤如下：
     1. **计算缩放因子 (**$\gamma$**)**：计算整个权重矩阵的平均绝对值：$\gamma = \frac{1}{nm} \sum_{ij} |W_{ij}|$。
     2. **缩放与圆整**：将权重除以 $\gamma$ 并四舍五入到最近整数：$\tilde{W} = \text{RoundClip}(\frac{W}{\gamma + \epsilon}, -1, 1)$。
     3. **参与计算**：使用量化后的 $\tilde{W}$ 进行矩阵乘法，计算 Loss。
 	- **直通估计器 (STE) 反向传播 (Backward Pass)**： 由于 `Round` 函数不可导，本项目采用 **Straight-Through Estimator (STE)** 技术。在反向传播时，忽略量化操作，直接将 $\tilde{W}$ 的梯度传递给原始浮点权重 $W$ 进行更新。这使得模型能够“感知”到量化带来的误差并自我修正。
- **8-bit 激活值动态量化 (Per-Token Activation Quantization)**： 针对多层网络中数据逐层传递的精度保持问题，设计 **“在线量化 (On-the-fly Quantization)”** 机制。
  - **挑战**：Transformer 每一层的输出（Int32/FP32）若直接传递给下一层，数据位宽会膨胀，导致带宽爆炸。
  - **解决方案**：在 TDPU 计算完成后或 CPU 后处理阶段，实施 **Per-Token 动态量化**。
        1. **最大值统计**：对当前 Token 的输出向量（Hidden State）寻找绝对最大值 $|Max|$。
        2. **动态缩放**：根据 $|Max|$ 计算缩放因子 $S = 127 / |Max|$。
        3. **重整化**：将高位宽数据 $X$ 转换为 $X_{int8} = \text{Round}(X \times S)$，使其回归 **Int8** 范围。
  - **价值**：确保流入下一层 TDPU 的输入数据始终为 8-bit，维持“Int8 输入 $\times$ 1.58-bit 权重”的高效计算模式，防止多层累积误差。

- **驱动适配**：编写 C 语言驱动，负责将量化权重加载至 DDR，并控制 RISC-V 进行推理调度。

# 4. 创新点与项目特色 (Innovation Points)

**（1）架构创新：完全无乘法器的计算数据通路**  
本项目打破了传统 AI 芯片依赖 DSP 乘法单元的定式。针对 1.58-bit 权重的数学特性，在硬件底层将昂贵的乘法运算转化为低成本的**移位、取反与加法选择逻辑**。这种设计不仅大幅减少了 FPGA 的逻辑资源占用（完全不需要用到 DSP 这种奢侈的计算单元），更显著提升了单位面积的算力密度，是针对 Transformer 线性层计算特性的极致优化。
**（2）设计创新：在同一片 FPGA 上集成 CPU 和 TDPU**  
**（3）系统创新：基于“复杂逻辑软件化”的软硬协同分流**  
不同于全硬件实现的复杂加速器，本项目提出“复杂逻辑软件化，计算密集硬件化”的轻量级 SoC 架构。

- 利用 **RISC-V CPU** 的通用性与灵活性，处理 Embedding 查表、Softmax 归一化等逻辑复杂但计算量小的算子；
- 利用 **FPGA** 的并行性，专攻计算量巨大的矩阵乘法。  
    这种分工既降低了硬件开发的工程难度，又保证了系统对不同 Transformer 变体模型的兼容性。

# 5. 项目研究进度安排

 **第一阶段：IP 核开发 (SystemVerilog)**

1. **TDPU 开发**：完成三值点积阵列的 RTL 编写，通过 Verilator 仿真验证与 Python 黄金模型的一致性。
2. **RISC-V 开发**：完成取指、译码、执行等流水线模块，编写 **AXI4-Lite Master** 接口模块，并通过仿真验证对寄存器的读写能力。

**第二阶段：SoC 系统集成**

1. **Zynq 配置**：在 Vivado 中实例化 Zynq PS 核，开启 **DDR 控制器** 和 **AXI HP Slave** 端口。
2. **互联构建**：添加 **AXI DMA** 和 **AXI Interconnect** IP 核，连接 PS 端的 DDR 和 PL 端的 TDPU。
3. **时钟域规划**：配置 PL 端时钟（如 100MHz），处理好 PS 与 PL 之间的复位同步

**第三阶段：Qwen 模型适配**

1. **模型量化适配 (Quantization-Aware Fine-tuning)**：基于 **Qwen2.5-0.5B-Instruct** 预训练模型，在 PyTorch 中使用开源数据集进行 **1.58-bit 量化感知微调 (QAT)**。通过微调恢复直接量化带来的精度损失，确保生成的三值权重具备流畅的对话能力，并最终导出二进制权重文件。

**第四阶段：板级验证与演示**

1. **ARM 侧验证：** 先编写 ARM 裸机程序，通过 DMA 往 TDPU 发送测试矩阵，验证硬件通路畅通。
2. **RISC-V 侧部署**：将 RISC-V 软核接入系统总线，接管控制权。
3. **最终演示**：上位机（PC）通过串口发送 "你好"，FPGA 实时计算并返回 "你好！我是基于 RISC-V 的 AI 助手..."，并在屏幕上流畅显示。。

**预期成果**：
**实物系统：** 一套基于 **Xilinx Zynq-7020 开发板** 的 1.58-bit AI 加速 SoC 演示系统。
**硬件源码：**

1. 通用的 **AXI-Stream 接口三值加速器 (TDPU)** RTL 源码。
2. 带有 **AXI-Lite 接口的 RISC-V 处理器** RTL 源码。
3. 完整的 Vivado 工程文件（含 Block Design 连线图）。
**技术文档**：《基于 Zynq 的 RISC-V/ARM 双轨架构设计与性能分析报告》。

# 6. 已有基础

## 与本项目有关的研究积累和已取得的成绩

1. 项目团队成员均为电子信息类专业本科生，已系统修读《数字电路与逻辑设计》《信号与系统》等基础课程，熟悉 git 项目管理流程和 linux 命令行基本操作，熟练使用 Ai 编程助手（如 gemini, vscode copilot, antigravity），正在自学硬件描述语言（Verilog/SystemVerilog）编程、计算机体系结构、数字系统设计流程及 Transformer 模型基础理论。
2. 项目团队曾在 2025 年高教社杯全国大学生数学建模竞赛中获得国家级二等奖，在 2025 年全国大学生电子设计竞赛中获得省级三等奖，项目合作经验丰富，具有良好的团队协作能力。部分团队成员在 2025 年中山大学第一届人工智能挑战赛中获得校级一等奖，具有一定的人工智能深度学习理论基础；项目团队均报名了中科院计算机所的开源芯片计划“一生一芯”，目前队长已完成预学习阶段中的 F 阶段和 E 阶段前 20%。
3. 前期实践中，团队成员已完成相关技术验证项目：利用 PyTorch 完成小型神经网络模型的训练与三值量化实验，熟悉模型权重处理流程；同时，团队已深入研读 BitNet b 1.58 相关论文，梳理出三值量化计算、分块矩阵运算的核心技术路径，为项目开展奠定了坚实的理论与实践基础。
4. [项目仓库](https://github.com/silence-breaker/Risc-V-AI-FPGA-Program)
