import os
# ==========================================
# ğŸš¨ å¼ºåˆ¶ä¿®å¤ï¼šåœ¨è¿™é‡ŒæŒ‡å®šä½ çš„ D ç›˜è·¯å¾„
# æ³¨æ„ï¼šä½ çš„æ–‡ä»¶å¤¹æ˜¯ AI_Workï¼Œä¸æ˜¯ AI_Benchmark
os.environ["HF_HOME"] = "/mnt/d/AI_Work/hf_cache"
# ==========================================

import torch
import torch.nn as nn
import time
# ... (åé¢çš„ä»£ç ä¿æŒä¸å˜)
import psutil
import os
import pandas as pd
import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
MODEL_ID = "microsoft/bitnet-b1.58-2B-4T-bf16"
REPORT_FILENAME = "BitNet_Analysis_Report.md"
# ä¸ºäº†ç¡®ä¿åœ¨æ™®é€š CPU ä¸Šä¹Ÿèƒ½è·‘ï¼Œæˆ‘ä»¬å°†ç²¾åº¦è½¬ä¸º float32ï¼Œ
# å¦‚æœä½ çš„ CPU æ”¯æŒ AVX-512 BF16ï¼Œå¯ä»¥æ”¹ä¸º torch.bfloat16
DTYPE = torch.float32 
DEVICE = "cpu"

# ================= å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æ•°æ® =================
layer_stats = []
model_structure_str = ""

# ================= è¾…åŠ©å‡½æ•° =================
def get_memory_mb():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def format_params(num):
    if num > 1e6:
        return f"{num/1e6:.2f}M"
    return str(num)

# ================= é’©å­å‡½æ•° (The Spies) =================
# æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªé’©å­ï¼šä¸€ä¸ªåœ¨è¿›å…¥å±‚ä¹‹å‰è®°å½•æ—¶é—´/å†…å­˜ï¼Œä¸€ä¸ªåœ¨å‡ºæ¥åè®°å½•å¢é‡

pre_record = {}

def pre_forward_hook(module, input):
    # è®°å½•æ¨¡å—çš„å†…å­˜ IDï¼Œé˜²æ­¢é€’å½’è°ƒç”¨æ··æ·†
    module_id = id(module)
    pre_record[module_id] = {
        "start_time": time.perf_counter(),
        "start_mem": get_memory_mb()
    }

def post_forward_hook(name, layer_type):
    def hook(module, input, output):
        module_id = id(module)
        if module_id in pre_record:
            end_time = time.perf_counter()
            end_mem = get_memory_mb()
            start_data = pre_record[module_id]
            
            duration_ms = (end_time - start_data["start_time"]) * 1000
            mem_delta = end_mem - start_data["start_mem"]
            
            # è®¡ç®—è¿ç®—é‡ (ç®€å•ä¼°ç®—çŸ©é˜µä¹˜æ³•)
            # Linear å±‚è¿ç®—é‡ â‰ˆ 2 * In * Out * Batch * Seq
            ops = 0
            input_shape = "N/A"
            if isinstance(input, tuple) and len(input) > 0:
                if isinstance(input[0], torch.Tensor):
                    shape = input[0].shape
                    input_shape = str(list(shape))
                    # å‡è®¾ input[0] æ˜¯ [batch, seq, in_features]
                    if isinstance(module, nn.Linear) and len(shape) == 3:
                        batch, seq, in_feat = shape
                        out_feat = module.out_features
                        ops = 2 * batch * seq * in_feat * out_feat
            
            layer_stats.append({
                "Layer Name": name,
                "Type": layer_type,
                "Input Shape": input_shape,
                "Time (ms)": duration_ms,
                "Mem Delta (MB)": mem_delta,
                "OPs": ops
            })
            
            # æ¸…ç†è®°å½•
            del pre_record[module_id]
    return hook

# ================= ä¸»ç¨‹åº =================
def main():
    global model_structure_str
    print(f"ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹: {MODEL_ID} ...")
    start_load = time.time()
    
    # 1. åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=DTYPE).to(DEVICE)
    
    load_time = time.time() - start_load
    model_mem = get_memory_mb()
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆã€‚è€—æ—¶: {load_time:.2f}s, å½“å‰å†…å­˜: {model_mem:.2f} MB")
    
    # ä¿å­˜æ¨¡å‹ç»“æ„å­—ç¬¦ä¸²ç”¨äºæŠ¥å‘Š
    model_structure_str = str(model)

    # 2. è‡ªåŠ¨æŒ‚è½½é’©å­ (Auto-Instrumentation)
    print("ğŸ”§ æ­£åœ¨ç»™æ¨¡å‹å±‚æŒ‚è½½æ€§èƒ½ç›‘æµ‹é’©å­...")
    hooks = []
    for name, module in model.named_modules():
        # åªç›‘æµ‹å…³é”®å±‚ï¼šLinear (Projections) å’Œ Norm
        # è¿‡æ»¤æ‰å¤§å®¹å™¨ï¼Œåªçœ‹å¶å­èŠ‚ç‚¹
        if isinstance(module, nn.Linear):
            # åŒºåˆ†æ˜¯ Attention é‡Œçš„è¿˜æ˜¯ MLP é‡Œçš„
            h1 = module.register_forward_pre_hook(pre_forward_hook)
            h2 = module.register_forward_hook(post_forward_hook(name, "Linear"))
            hooks.extend([h1, h2])
        elif "Norm" in module.__class__.__name__:
            h1 = module.register_forward_pre_hook(pre_forward_hook)
            h2 = module.register_forward_hook(post_forward_hook(name, "Norm"))
            hooks.extend([h1, h2])
        elif "Act" in module.__class__.__name__ or "SiLU" in module.__class__.__name__:
             h1 = module.register_forward_pre_hook(pre_forward_hook)
             h2 = module.register_forward_hook(post_forward_hook(name, "Activation"))
             hooks.extend([h1, h2])

    # 3. è¿è¡Œæ¨ç† (Run Inference)
    print("ğŸƒ å¼€å§‹æ¨ç†æµ‹è¯• (Prompt: 'BitNet is')...")
    input_text = "BitNet is"
    inputs = tokenizer(input_text, return_tensors="pt").to(DEVICE)
    
    # é¢„çƒ­ä¸€æ¬¡ (Warmup) - è®© PyTorch åˆå§‹åŒ–å†…å­˜æ± å’Œ JIT
    with torch.no_grad():
        model(**inputs)
    
    # æ¸…ç©ºé¢„çƒ­æ•°æ®ï¼Œé‡æ–°è®°å½•
    layer_stats.clear()
    
    # æ­£å¼è¿è¡Œ
    with torch.no_grad():
        outputs = model(**inputs)

    print("ğŸ“Š æ•°æ®æ”¶é›†å®Œæ¯•ï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
    generate