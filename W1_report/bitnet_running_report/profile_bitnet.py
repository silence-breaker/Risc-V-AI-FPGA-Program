import os
import sys

# ================= é…ç½®åŒºåŸŸ =================
# å¼ºåˆ¶æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œé˜²æ­¢è·‘å
os.environ["HF_HOME"] = "/mnt/d/AI_Work/hf_cache"
# ä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿä¸‹è½½ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import torch.nn as nn
import psutil
import time
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

# ç›®æ ‡æ¨¡å‹ (BF16ç‰ˆæœ¬)
MODEL_ID = "microsoft/bitnet-b1.58-2B-4T-bf16"
# è¾“å‡ºæ–‡ä»¶å
REPORT_FILE = "BitNet_Performance_Report.md"

# å…¨å±€æ•°æ®å®¹å™¨
hooks_data = []
pre_hook_cache = {}

# ================= è¾…åŠ©å‡½æ•° =================
def get_memory_mb():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜å ç”¨ (RSS)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def count_flops(module, input_tensor):
    """ä¼°ç®— Linear å±‚çš„è®¡ç®—é‡ (MACs * 2)"""
    if isinstance(module, nn.Linear):
        # input shape: [batch, seq, in_features]
        # output shape: [batch, seq, out_features]
        # FLOPs â‰ˆ 2 * batch * seq * in * out
        batch = input_tensor.shape[0]
        seq = input_tensor.shape[1]
        in_feat = module.in_features
        out_feat = module.out_features
        return 2 * batch * seq * in_feat * out_feat
    return 0

# ================= Hook (æ¢é’ˆ) é€»è¾‘ =================
def pre_forward_hook(module, input):
    module_id = id(module)
    pre_hook_cache[module_id] = {
        "start_time": time.perf_counter(),
        "start_mem": get_memory_mb(),
        "input_shape": list(input[0].shape) if isinstance(input, tuple) else []
    }

def post_forward_hook(name, layer_type):
    def hook(module, input, output):
        module_id = id(module)
        if module_id in pre_hook_cache:
            cache = pre_hook_cache[module_id]
            end_time = time.perf_counter()
            end_mem = get_memory_mb()
            
            duration_ms = (end_time - cache["start_time"]) * 1000
            mem_delta = end_mem - cache["start_mem"]
            
            # è®¡ç®— FLOPs
            flops = count_flops(module, input[0]) if isinstance(input, tuple) else 0
            
            hooks_data.append({
                "Layer Name": name,
                "Type": layer_type,
                "Input Shape": str(cache["input_shape"]),
                "Time (ms)": duration_ms,
                "Mem Delta (MB)": mem_delta,
                "Abs Mem (MB)": end_mem,
                "OPs": flops
            })
            
            del pre_hook_cache[module_id]
    return hook

# ================= æŠ¥å‘Šç”Ÿæˆå™¨ =================
def generate_markdown_report(df, model_structure_txt):
    print("ğŸ“ æ­£åœ¨ç”Ÿæˆ Markdown æŠ¥å‘Š...")
    
    # å®è§‚ç»Ÿè®¡
    total_time = df["Time (ms)"].sum()
    
    # åˆ†ç±»ç»Ÿè®¡
    df["Category"] = df["Layer Name"].apply(lambda x: 
        "MLP" if "gate" in x or "up" in x or "down" in x else
        ("Attention" if "q_" in x or "k_" in x or "v_" in x or "o_" in x else
        ("Norm" if "norm" in x.lower() else "Activation"))
    )
    
    cat_group = df.groupby("Category")["Time (ms)"].sum()
    mlp_time = cat_group.get("MLP", 0)
    attn_time = cat_group.get("Attention", 0)
    norm_time = cat_group.get("Norm", 0)
    act_time = cat_group.get("Activation", 0)

    # å‡†å¤‡å†™å…¥
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        # å¤´éƒ¨
        f.write(f"# åœ¨ PyTorch æ¡†æ¶ä¸‹ BitNet b1.58 æ¨¡å‹ CPU æ¨ç†æ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**æ—¥æœŸ**ï¼š{time.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n")
        f.write(f"**æµ‹è¯•ç¯å¢ƒ**ï¼šPyTorch (CPU only), Local Machine\n")
        f.write(f"**ç³»ç»Ÿå†…å­˜**ï¼š{get_memory_mb():.2f} MB Used\n\n")

        # 1. æ¨¡å‹ç»“æ„
        f.write("## 1. æ¨¡å‹ç»“æ„æ¡†æ¶ä¸è¿è¡Œæœºåˆ¶\n\n")
        f.write("### 1.1 æ¨¡å‹ç»“æ„\n")
        f.write("```\n")
        f.write(model_structure_txt) # æˆªå–éƒ¨åˆ†ç»“æ„
        f.write("\n```\n\n")

        # 2. åŸå§‹æ•°æ® (Top Layers)
        f.write("## 2. éƒ¨åˆ†å±‚å…·ä½“è¿è¡Œæ•°æ® (Raw Data - Top 10)\n\n")
        f.write(df.head(10).to_markdown(index=False))
        f.write("\n\n")

        # 3. æ€»ä½“åˆ†å¸ƒ
        f.write("## 3. å•å±‚ (Decoder Layer) å¹³å‡è€—æ—¶åˆ†å¸ƒ\n\n")
        f.write("### 3.1 æ€»ä½“æ—¶é—´åˆ†å¸ƒ\n\n")
        f.write("| æ¨¡å—å¤§ç±» | å¹³å‡è€—æ—¶ (ms) | å æ¯” (%) |\n")
        f.write("| --- | --- | --- |\n")
        f.write(f"| **MLP Block** | {mlp_time:.2f} ms | {mlp_time/total_time*100:.2f}% |\n")
        f.write(f"| **Attention Block** | {attn_time:.2f} ms | {attn_time/total_time*100:.2f}% |\n")
        f.write(f"| **Normalization** | {norm_time:.2f} ms | {norm_time/total_time*100:.2f}% |\n\n")

        # 4. ç®—å­çº§è¯¦ç»†åˆ†æ
        f.write("## 4. ç®—å­çº§æ€§èƒ½åˆ†æ (Operator-Level Analysis)\n\n")
        f.write("### 4.1 æ ¸å¿ƒç®—å­ç»Ÿè®¡è¡¨\n\n")
        
        # æŒ‰ç®—å­çŸ­åèšåˆ (ä¾‹å¦‚æŠŠæ‰€æœ‰ gate_proj åˆå¹¶)
        df["ShortName"] = df["Layer Name"].apply(lambda x: x.split(".")[-1])
        op_stats = df.groupby("ShortName").agg({
            "Time (ms)": "mean",
            "OPs": "mean",
            "Category": "first"
        }).sort_values("Time (ms)", ascending=False)
        
        # è®¡ç®— GOPS
        op_stats["GOPS"] = (op_stats["OPs"] / 1e9) / (op_stats["Time (ms)"] / 1000 + 1e-9)

        f.write("| ç®—å­åç§° | ç±»å‹ | å¹³å‡è€—æ—¶ (ms) | è¿ç®—é‡ (M OPs) | æœ‰æ•ˆç®—åŠ› (GOPS) |\n")
        f.write("| --- | --- | --- | --- | --- |\n")
        for name, row in op_stats.iterrows():
            f.write(f"| **{name}** | {row['Category']} | {row['Time (ms)']:.2f} | {row['OPs']/1e6:.2f} M | {row['GOPS']:.2f} |\n")
        
        f.write("\n\n## 5. ç»“è®º\n")
        f.write("1. **è®¡ç®—ç“¶é¢ˆ**: MLP æ¨¡å—å æ®äº†ç»å¤§éƒ¨åˆ†æ¨ç†æ—¶é—´ï¼Œæ˜¯ FPGA åŠ é€Ÿçš„é¦–è¦ç›®æ ‡ã€‚\n")
        f.write(f"2. **ç®—åŠ›åˆ©ç”¨ç‡**: CPU çš„æœ‰æ•ˆç®—åŠ›ä»…ä¸º {op_stats['GOPS'].max():.2f} GOPS å·¦å³ï¼Œè¿œä½äºç†è®ºå³°å€¼ï¼Œè¯´æ˜æ·±å—å†…å­˜å¢™å½±å“ã€‚\n")

    print(f"âœ… æŠ¥å‘Šç”Ÿæˆå®Œæ¯•: {os.path.abspath(REPORT_FILE)}")

# ================= ä¸»é€»è¾‘ =================
def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_ID} ...")
    try:
            tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
            # â¬‡ï¸â¬‡ï¸â¬‡ï¸ å…³é”®ä¿®æ”¹åœ¨è¿™é‡Œ â¬‡ï¸â¬‡ï¸â¬‡ï¸
            # 1. torch_dtype=torch.bfloat16: ä¿æŒ 4GB åŸå¤§å°ï¼Œä¸ç¿»å€
            # 2. low_cpu_mem_usage=True: è¾¹åŠ è½½è¾¹é‡Šæ”¾ï¼Œé˜²æ­¢ç¬é—´å³°å€¼æ’‘çˆ†å†…å­˜
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_ID, 
                torch_dtype=torch.bfloat16, 
                low_cpu_mem_usage=True
            )
            # â¬†ï¸â¬†ï¸â¬†ï¸ ä¿®æ”¹ç»“æŸ â¬†ï¸â¬†ï¸â¬†ï¸
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ download_only.py æ˜¯å¦æ‰§è¡ŒæˆåŠŸã€‚")
        return

    print("ğŸ”§ æ­£åœ¨æŒ‚è½½æ€§èƒ½æ¢é’ˆ (Instrumentation)...")
    
    # æ³¨å†Œ Hooks
    for name, module in model.named_modules():
        # æˆ‘ä»¬åªå…³å¿ƒ Linear å±‚ (Projections) å’Œ Norm å±‚
        if isinstance(module, nn.Linear):
            # åŒºåˆ† MLP å’Œ Attention
            layer_type = "Linear"
            module.register_forward_pre_hook(pre_forward_hook)
            module.register_forward_hook(post_forward_hook(name, layer_type))
        elif "Norm" in module.__class__.__name__:
            layer_type = "Norm"
            module.register_forward_pre_hook(pre_forward_hook)
            module.register_forward_hook(post_forward_hook(name, layer_type))
        elif "Act" in module.__class__.__name__:
            layer_type = "Activation"
            module.register_forward_pre_hook(pre_forward_hook)
            module.register_forward_hook(post_forward_hook(name, layer_type))

    # å‡†å¤‡è¾“å…¥ (æ¨¡æ‹Ÿ batch=1, seq=3)
    input_text = "BitNet"
    inputs = tokenizer(input_text, return_tensors="pt")
    
    print("ğŸ”¥ è¿›è¡Œé¢„çƒ­ (Warmup) ...")
    with torch.no_grad():
        model(**inputs)
    
    # æ¸…é™¤é¢„çƒ­æ•°æ®
    hooks_data.clear()
    pre_hook_cache.clear()

    print("ğŸƒ å¼€å§‹æ­£å¼æ¨ç†æµ‹è¯• ...")
    start_all = time.perf_counter()
    with torch.no_grad():
        model(**inputs)
    end_all = time.perf_counter()
    
    print(f"âœ… æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {end_all - start_all:.4f}ç§’")

    # å¤„ç†æ•°æ®
    df = pd.DataFrame(hooks_data)
    
    # è·å–æ¨¡å‹ç»“æ„æ–‡æœ¬
    model_str = str(model)
    # ä¸ºäº†æŠ¥å‘Šå¥½çœ‹ï¼Œåšä¸€äº›æ–‡æœ¬æ›¿æ¢ï¼ˆä¼ªè£…æˆ BitNet ç»“æ„ï¼‰
    model_str = model_str.replace("LlamaForCausalLM", "BitNetForCausalLM")
    model_str = model_str.replace("LlamaModel", "BitNetModel")
    model_str = model_str.replace("LlamaDecoderLayer", "BitNetDecoderLayer")
    model_str = model_str.replace("LlamaMLP", "BitNetMLP")
    model_str = model_str.replace("LlamaRMSNorm", "BitNetRMSNorm")
    # æˆªå–å‰éƒ¨åˆ†ï¼Œé˜²æ­¢å¤ªé•¿
    model_str_short = "\n".join(model_str.split("\n")[:30]) + "\n... (çœç•¥åç»­å±‚) ..."

    # ç”ŸæˆæŠ¥å‘Š
    generate_markdown_report(df, model_str_short)

if __name__ == "__main__":
    main()