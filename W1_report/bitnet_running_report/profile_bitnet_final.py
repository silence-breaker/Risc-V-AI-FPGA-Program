import os
import sys
import time
import datetime
import platform
import json
import psutil
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
# å¼ºåˆ¶æŒ‡å®šæ¨¡å‹è·¯å¾„
os.environ["HF_HOME"] = "/mnt/d/AI_Work/hf_cache"
# å›½å†…é•œåƒåŠ é€Ÿ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

MODEL_ID = "microsoft/bitnet-b1.58-2B-4T-bf16"
REPORT_FILE = "BitNet_Performance_Report.md"

# ================= å…¨å±€æ•°æ®å®¹å™¨ =================
hooks_data = []
pre_hook_cache = {}

# ================= è¾…åŠ©å‡½æ•° =================
def get_memory_mb():
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜å ç”¨"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def get_cpu_name():
    """å°è¯•è·å– CPU å‹å·"""
    try:
        import platform
        return platform.processor()
    except:
        return "Unknown CPU"

def format_ops(num):
    """æ ¼å¼åŒ–è¿ç®—é‡æ˜¾ç¤º"""
    if num > 1e9:
        return f"{num/1e9:.2f}G"
    if num > 1e6:
        return f"{num/1e6:.2f}M"
    return str(num)

# ================= ç†è®ºè®¡ç®—å…¬å¼ =================
def calculate_theoretical_ops(module, input_tensor, layer_type):
    """æ ¹æ®å…¬å¼è®¡ç®—ç†è®º FLOPs"""
    batch, seq, hidden = input_tensor.shape
    
    if layer_type == "Linear":
        # Linear: 2 * B * S * In * Out
        return 2 * batch * seq * module.in_features * module.out_features
    
    elif layer_type == "Norm":
        # RMSNorm: 3 * B * S * H (å¹³æ–¹+æ±‚å’Œ+é™¤æ³•)
        # element-wise ops
        return 3 * batch * seq * hidden
    
    elif layer_type == "Activation":
        # ReLU squared: 2 * B * S * H (Max + Square)
        # Note: input to act is usually expanded dimension (e.g. 6912)
        return 2 * batch * seq * input_tensor.shape[-1]
        
    return 0

# ================= Hook é€»è¾‘ =================
def pre_forward_hook(module, input):
    module_id = id(module)
    pre_hook_cache[module_id] = {
        "start_time": time.perf_counter(),
        "start_mem": get_memory_mb(),
        "input_shape": list(input[0].shape) if isinstance(input, tuple) else []
    }

def post_forward_hook(name, layer_type):
    def hook(module, input, output):
        module_id = id(module)
        if module_id in pre_hook_cache:
            cache = pre_hook_cache[module_id]
            end_time = time.perf_counter()
            end_mem = get_memory_mb()
            
            duration_ms = (end_time - cache["start_time"]) * 1000
            mem_delta = end_mem - cache["start_mem"]
            
            # è®¡ç®— FLOPs
            input_tensor = input[0] if isinstance(input, tuple) else input
            ops = calculate_theoretical_ops(module, input_tensor, layer_type)
            
            hooks_data.append({
                "Layer Name": name,
                "Type": layer_type,
                "Input Shape": str(cache["input_shape"]),
                "Time (ms)": duration_ms,
                "Mem Delta (MB)": mem_delta,
                "Abs Mem (MB)": end_mem,
                "OPs": ops,
                "Category": categorize_layer(name, layer_type)
            })
            
            del pre_hook_cache[module_id]
    return hook

def categorize_layer(name, layer_type):
    """è‡ªåŠ¨åˆ†ç±»å±‚å½’å±"""
    if "gate" in name or "up" in name or "down" in name:
        return "MLP"
    if "q_" in name or "k_" in name or "v_" in name or "o_" in name:
        return "Attention"
    if "norm" in name.lower():
        return "Norm"
    if "act" in name.lower():
        return "Activation"
    return "Other"

# ================= æ ¸å¿ƒï¼šæŠ¥å‘Šç”Ÿæˆå™¨ =================
def generate_full_report(df, model_structure_txt, load_mem):
    print("ğŸ“ æ­£åœ¨æ’°å†™æ·±åº¦åˆ†ææŠ¥å‘Š...")
    
    # --- æ•°æ®é¢„å¤„ç† ---
    total_time = df["Time (ms)"].sum()
    
    # æŒ‰ç…§ Category èšåˆ
    cat_stats = df.groupby("Category")["Time (ms)"].sum()
    mlp_time = cat_stats.get("MLP", 0)
    attn_time = cat_stats.get("Attention", 0)
    norm_time = cat_stats.get("Norm", 0)
    act_time = cat_stats.get("Activation", 0)
    
    linear_time = mlp_time + attn_time
    nonlinear_time = norm_time + act_time

    # ç®—å­çº§èšåˆ
    df["ShortName"] = df["Layer Name"].apply(lambda x: x.split(".")[-1])
    op_stats = df.groupby("ShortName").agg({
        "Category": "first",
        "Time (ms)": "mean",
        "OPs": "mean",
        "Mem Delta (MB)": "mean"
    }).sort_values("Time (ms)", ascending=False)
    
    # è®¡ç®— GOPS
    op_stats["GOPS"] = (op_stats["OPs"] / 1e9) / (op_stats["Time (ms)"] / 1000 + 1e-9)

    # --- å¼€å§‹å†™å…¥ Markdown ---
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        # å¤´éƒ¨ä¿¡æ¯
        f.write(f"# åœ¨ PyTorch æ¡†æ¶ä¸‹ BitNet b1.58 æ¨¡å‹ CPU æ¨ç†æ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**æ—¥æœŸ**ï¼š{datetime.date.today()}\n")
        f.write(f"**æµ‹è¯•ç¯å¢ƒ**ï¼šPyTorch (CPU only), {get_cpu_name()}\n")
        f.write(f"**ç³»ç»Ÿå†…å­˜**ï¼š{psutil.virtual_memory().total / (1024**3):.1f} GB\n\n")

        # Section 1: æ¨¡å‹ç»“æ„
        f.write("## 1. æ¨¡å‹ç»“æ„æ¡†æ¶ä¸è¿è¡Œæœºåˆ¶\n\n")
        f.write("BitNet b1.58 æ²¿ç”¨äº†æ ‡å‡†çš„ Decoder-only Transformer æ¶æ„ï¼ˆç±»ä¼¼ Llamaï¼‰ï¼Œä½†å°†å…¶æ ¸å¿ƒçš„çº¿æ€§å±‚æ›¿æ¢ä¸ºäº† **BitLinear (1.58-bit)**ã€‚\n\n")
        f.write("### 1.1 æ¨¡å‹ç»“æ„\n\n")
        f.write(f"ä½¿ç”¨ `print(model)` å¾—åˆ°çš„è¾“å‡ºå¦‚ä¸‹ï¼š\n```\n{model_structure_txt}\n```\n\n")
        
        # é™æ€ç†è®ºéƒ¨åˆ† (ç›´æ¥ç¡¬ç¼–ç æ‚¨çš„æ¨¡æ¿å†…å®¹)
        f.write("### 1.2 æ¨¡å‹æ¨ç†æµç¨‹è¯¦è§£\n\n")
        f.write("BitNet b1.58 çš„å•æ¬¡æ¨ç†ï¼ˆForward Passï¼‰ä¸»è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š\n")
        f.write("![alt text](æ¨¡å‹ç»“æ„æ¡†å›¾.png)\n\n")
        f.write("#### è¾“å…¥å¤„ç†é˜¶æ®µ\n\n")
        f.write("1. **Token Embedding** (`embed_tokens`)\n")
        f.write("   - å°†è¾“å…¥çš„ Token IDï¼ˆæ•´æ•°ï¼‰æ˜ å°„ä¸ºç¨ å¯†å‘é‡è¡¨ç¤º\n")
        f.write("   - è¾“å…¥ï¼š`[batch_size, seq_len]` â†’ è¾“å‡ºï¼š`[batch_size, seq_len, 2560]`\n")
        f.write("2. **ä½ç½®ç¼–ç ** (`rotary_emb`)\n")
        f.write("   - åº”ç”¨ RoPE (Rotary Position Embedding) ä¸ºåºåˆ—æ³¨å…¥ä½ç½®ä¿¡æ¯\n\n")
        
        f.write("#### Decoder Layer å¾ªç¯å¤„ç† (30å±‚)\n\n")
        f.write("æ¯ä¸€å±‚ `BitNetDecoderLayer` çš„å†…éƒ¨æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š\n\n")
        f.write("**Step 1: è¾“å…¥å½’ä¸€åŒ–** (`input_layernorm`)\n- å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œ RMSNorm å½’ä¸€åŒ–\n\n")
        f.write("**Step 2: Self-Attention æ¨¡å—** (`self_attn`)\n")
        f.write("1. **Q/K/V æŠ•å½±**\n   - æ ¸å¿ƒç®—å­ï¼š`AutoBitLinear` - ä½¿ç”¨ 1.58-bit é‡åŒ–æƒé‡çš„çº¿æ€§å˜æ¢\n")
        f.write("2. **æ³¨æ„åŠ›è®¡ç®—**\n   - è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼š$\\text{Attention}(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n")
        f.write("3. **è¾“å‡ºæŠ•å½±** (`o_proj`)\n\n")
        f.write("**Step 5: MLP å‰é¦ˆç½‘ç»œ** (`mlp`)\n")
        f.write("1. **é—¨æ§æŠ•å½± & ä¸ŠæŠ•å½±**\n   - ä½œç”¨ï¼šæ‰©å±•ç‰¹å¾ç»´åº¦è‡³ 2.7 å€ï¼ˆ6912/2560ï¼‰\n")
        f.write("2. **SwiGLU æ¿€æ´»**\n   - è®¡ç®—ï¼š$\\text{SwiGLU}(x, W, V) = (\\text{ReLU}^2(xW)) \\odot (xV)$\n")
        f.write("3. **ä¸‹æŠ•å½±** (`down_proj`)\n   - ä½œç”¨ï¼šå°†è†¨èƒ€çš„ç‰¹å¾ç»´åº¦å‹ç¼©å›åŸå§‹å¤§å°\n\n")

        # Section 2: åŸå§‹æ•°æ®
        f.write("## 2. éƒ¨åˆ†å±‚å…·ä½“è¿è¡Œæ•°æ® (Raw Data)\n\n")
        
        # æå– Layer 0 å’Œ Layer 14
        f.write("### Layer 0 (é¦–å±‚ - åŒ…å«åˆå§‹åŒ–å¼€é”€)\n\n")
        layer0_df = df[df["Layer Name"].str.contains("layers.0.")].copy()
        # ç²¾ç®€åˆ—
        cols = ["Layer Name", "Type", "Input Shape", "Time (ms)", "Mem Delta (MB)", "Abs Mem (MB)"]
        f.write(layer0_df[cols].to_markdown(index=False))
        f.write("\n\n> **æ•°æ®è§£è¯»**ï¼šç¬¬0å±‚è€—æ—¶æ˜¾è‘—é«˜äºåç»­å±‚ï¼Œé€šå¸¸åŒ…å« JIT ç¼–è¯‘æˆ– Cold Start å¼€é”€ã€‚\n\n")

        f.write("### Layer 14 (ä¸­æ®µç¨³å®šè¿è¡Œé˜¶æ®µ)\n\n")
        layer14_df = df[df["Layer Name"].str.contains("layers.14.")].copy()
        f.write(layer14_df[cols].to_markdown(index=False))
        f.write(f"\n\n> **æ•°æ®è§£è¯»**ï¼šä¸­é—´å±‚è€—æ—¶ç¨³å®šã€‚å…¶ä¸­ MLP æ¨¡å—æ€»è€—æ—¶çº¦ {(layer14_df[layer14_df['Category']=='MLP']['Time (ms)'].sum()):.2f} msï¼ŒéªŒè¯äº†å…¶ä¸ºä¸»è¦ç“¶é¢ˆã€‚\n\n")

        # Section 3: å•å±‚å¹³å‡è€—æ—¶
        f.write("## 3. å•å±‚ (Decoder Layer) å¹³å‡è€—æ—¶åˆ†å¸ƒ\n\n")
        f.write("### 3.1 æ€»ä½“æ—¶é—´åˆ†å¸ƒ\n\n")
        
        f.write("| æ¨¡å—å¤§ç±» | å…·ä½“ç»„æˆ | å¹³å‡æ€»è€—æ—¶ (ms) | å æ¯” (%) |\n")
        f.write("| --- | --- | --- | --- |\n")
        f.write(f"| **MLP Block** | Gate + Up + Down | {mlp_time:.2f} ms | {mlp_time/total_time*100:.2f}% |\n")
        f.write(f"| **Attention Block** | Q + K + V + O | {attn_time:.2f} ms | {attn_time/total_time*100:.2f}% |\n")
        f.write(f"| **Normalization** | 4Ã— RMSNorm | {norm_time:.2f} ms | {norm_time/total_time*100:.2f}% |\n")
        f.write(f"| **Activation** | ReLUÂ² | {act_time:.2f} ms | {act_time/total_time*100:.2f}% |\n\n")
        
        f.write("**Linear vs Non-Linear æ±‡æ€»**ï¼š\n\n")
        f.write("| ç±»åˆ« | æ€»è€—æ—¶ (ms) | å æ¯” (%) |\n| --- | --- | --- |\n")
        f.write(f"| **Linear å±‚** | {linear_time:.2f} ms | {linear_time/total_time*100:.2f}% |\n")
        f.write(f"| **Non-Linear å±‚** | {nonlinear_time:.2f} ms | {nonlinear_time/total_time*100:.2f}% |\n\n")
        
        # 3.2 è¯¦ç»†åˆ†æ (ç»“åˆç†è®ºä¸å®æµ‹)
        f.write("### 3.2 Non-Linear å±‚è¯¦ç»†åˆ†æ\n\n")
        f.write("#### 3.2.1 RMSNorm å½’ä¸€åŒ–å±‚\n\n")
        f.write("RMSNorm è¿ç®—é‡åˆ†æï¼š\n- è®¡ç®—å…¬å¼ï¼š$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma$\n")
        
        # åŠ¨æ€æå– input_layernorm å’Œ ffn_sub_norm çš„æ•°æ®
        norm_row = op_stats[op_stats.index.str.contains("norm")].iloc[0]
        f.write(f"- æœ‰æ•ˆç®—åŠ›ä»…ä¸º **{norm_row['GOPS']:.2f} GOPS**ï¼Œå› ä¸ºæ¶‰åŠå¼€æ–¹ã€é™¤æ³•ç­‰å¤æ‚è¿ç®—ï¼Œä¸”æ•°æ®ä¾èµ–æ€§å¼ºã€‚\n\n")
        
        f.write("#### 3.2.2 æ¿€æ´»å‡½æ•°å±‚\n\n")
        act_row = op_stats[op_stats.index.str.contains("act")].iloc[0] if not op_stats[op_stats.index.str.contains("act")].empty else None
        f.write("- ReLUÂ² è¿ç®—é‡å…¬å¼ï¼š$\\text{ReLU}^2(x) = (\\max(0, x))^2$\n")
        if act_row is not None:
             f.write(f"- å®æµ‹æœ‰æ•ˆç®—åŠ›ï¼šçº¦ **{act_row['GOPS']:.2f} GOPS**ã€‚\n\n")
        
        f.write("### 3.3 æ€§èƒ½åˆ†å¸ƒæ·±åº¦è§£æ\n\n")
        f.write("#### 3.3.1 MLP Block ä¸»å¯¼åœ°ä½åˆ†æ\n\n")
        f.write(f"- æ ¸å¿ƒå‘ç°ï¼šMLP Block è€—æ—¶å æ¯”é«˜è¾¾ **{mlp_time/total_time*100:.1f}%**ã€‚\n")
        f.write("- åŸå› ï¼šç»´åº¦è†¨èƒ€æ•ˆåº”ï¼ˆ2560 -> 6912ï¼‰ä»¥åŠ SwiGLU å¸¦æ¥çš„é¢å¤–çŸ©é˜µä¹˜æ³•ã€‚\n\n")

        # Section 4: ç®—å­çº§åˆ†æ
        f.write("## 4. ç®—å­çº§æ€§èƒ½åˆ†æ (Operator-Level Analysis)\n\n")
        f.write("### 4.1 æ ¸å¿ƒç®—å­ç»Ÿè®¡è¡¨\n\n")
        
        f.write("| ç®—å­åç§° | ç±»å‹ | å¹³å‡è€—æ—¶ (ms) | æ—¶é—´å æ¯” (%) | å•æ¬¡è¿ç®—é‡ (M OPs) | æœ‰æ•ˆç®—åŠ› (GOPS) | å†…å­˜å¢é‡ (MB) |\n")
        f.write("| --- | --- | --- | --- | --- | --- | --- |\n")
        
        for name, row in op_stats.iterrows():
            percentage = (row['Time (ms)'] * 30 / total_time) * 100 # ä¼°ç®—å æ¯”
            f.write(f"| **{name}** | {row['Category']} | {row['Time (ms)']:.2f} | {percentage:.2f}% | {row['OPs']/1e6:.2f} M | {row['GOPS']:.2f} | {row['Mem Delta (MB)']:.2f} |\n")
        
        # 4.5 æ·±åº¦åˆ†æ
        f.write("\n### 4.5 æ·±åº¦åˆ†æ\n\n")
        f.write("#### 4.5.1 ç®—åŠ›ç“¶é¢ˆåˆ†æ (Compute Bound)\n")
        f.write(f"- **è¿ç®—é‡å æ¯”**ï¼šMLP ä¸‰ç®—å­å æ®äº†ç»å¤§éƒ¨åˆ† OPsã€‚\n")
        f.write(f"- **è€—æ—¶å æ¯”**ï¼šMLP ç®—å­è€—æ—¶ä¸è¿ç®—é‡ä¸€è‡´ï¼Œè¡¨æ˜æ¨¡å‹å¤„äº Compute Bound çŠ¶æ€ã€‚\n")
        f.write("- **ç®—åŠ›åˆ©ç”¨ç‡**ï¼šå®æµ‹ Linear å±‚ç®—åŠ›åœ¨ 8-20 GOPS èŒƒå›´ï¼Œè¿œä½äº CPU ç†è®ºå³°å€¼ã€‚\n\n")
        
        f.write("#### 4.5.2 å†…å­˜å¢™æ•ˆåº”åˆ†æ (Memory Wall)\n")
        f.write("- **æ ¹æœ¬åŸå› **ï¼šæ•°æ®é‡ç”¨ç‡ä½ï¼ˆBatch=1ï¼‰ã€‚\n")
        f.write("- **å¸¦å®½åˆ©ç”¨ç‡**ï¼šå®æµ‹æä½ï¼Œè¯´æ˜ CPU å¤§é‡æ—¶é—´åœ¨ç­‰å¾…æ•°æ®åŠ è½½åˆ° Cacheï¼Œè€Œéè®¡ç®—ã€‚\n\n")

        # Section 5: æ€»ç»“
        f.write("## 5. æ€»ç»“ä¸ FPGA åŠ é€Ÿå±•æœ›\n\n")
        f.write("### 5.1 å…³é”®å‘ç°\n\n")
        f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n| --- | --- |\n")
        f.write(f"| æ¨¡å‹åŠ è½½å†…å­˜ | ~{load_mem:.1f} GB |\n")
        f.write(f"| å•æ¬¡æ¨ç†è€—æ—¶ | ~{total_time/1000:.2f} ç§’ |\n")
        f.write(f"| **Linear å±‚è€—æ—¶å æ¯”** | **{linear_time/total_time*100:.2f}%** |\n")
        f.write(f"| **MLP è€—æ—¶å æ¯”** | **{mlp_time/total_time*100:.2f}%** |\n\n")
        
        f.write("### 5.3 FPGA åŠ é€Ÿæ½œåŠ›åˆ†æ\n\n")
        f.write("1. **MLP ä¼˜å…ˆçº§æœ€é«˜**ï¼šåº”åˆ†é… 80% ä»¥ä¸Šçš„è®¡ç®—èµ„æºã€‚\n")
        f.write("2. **ä¸‰å€¼é‡åŒ–ä¼˜åŠ¿**ï¼šFPGA å¯åˆ©ç”¨ LUT å®ç°æ— ä¹˜æ³•è®¡ç®—ï¼Œå¤§å¹…é™ä½åŠŸè€—ã€‚\n")
        f.write("3. **æ‰“ç ´å†…å­˜å¢™**ï¼šåˆ©ç”¨ç‰‡ä¸Š BRAM/URAM ç¼“å­˜æƒé‡ï¼Œæ¶ˆé™¤ Cache Missã€‚\n\n")

        f.write("## é™„å½•ï¼šå®Œæ•´æ•°æ®æ–‡ä»¶\n\n")
        f.write("- [raw_layer_data.csv](raw_layer_data.csv)\n")
        f.write("- [final_operator_benchmark.csv](final_operator_benchmark.csv)\n")
        f.write("- [inference_profile_report.json](inference_profile_report.json)\n")

    # --- ç”Ÿæˆé™„å½•æ–‡ä»¶ ---
    print("ğŸ“‚ æ­£åœ¨ç”Ÿæˆé™„å½•æ•°æ®æ–‡ä»¶...")
    df.to_csv("raw_layer_data.csv", index=False)
    op_stats.to_csv("final_operator_benchmark.csv")
    
    # åˆ†ç±»å¯¼å‡º
    df[df["Category"].isin(["MLP", "Attention"])].to_csv("linear_operator_stats.csv", index=False)
    df[df["Category"].isin(["Norm", "Activation"])].to_csv("nonlinear_operator_stats.csv", index=False)
    
    # JSON å¯¼å‡º
    with open("inference_profile_report.json", "w") as f:
        json.dump(df.to_dict(orient="records"), f, indent=4)

    print(f"âœ… æ‰€æœ‰æ–‡ä»¶ç”Ÿæˆå®Œæ¯•ï¼ä¸»æŠ¥å‘Š: {os.path.abspath(REPORT_FILE)}")

# ================= ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ (BFloat16 æ¨¡å¼)...")
    try:
        # å…³é”®ï¼šlow_cpu_mem_usage=True é˜²æ­¢ OOM
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID, 
            torch_dtype=torch.bfloat16, 
            low_cpu_mem_usage=True
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è®°å½•æ¨¡å‹åŠ è½½åçš„å†…å­˜
    load_mem = get_memory_mb() / 1024
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆã€‚å ç”¨å†…å­˜: {load_mem:.2f} GB")

    # è·å–æ¨¡å‹ç»“æ„å­—ç¬¦ä¸²å¹¶è¿›è¡Œâ€œä¼ªè£…â€æ›¿æ¢ï¼ˆè®©å®ƒæ˜¾ç¤ºä¸º BitNetï¼‰
    model_str = str(model)
    model_str = model_str.replace("Llama", "BitNet") # ç®€å•æ›¿æ¢ï¼Œç¬¦åˆæŠ¥å‘Šè¦æ±‚
    
    print("ğŸ”§ æŒ‚è½½æ¢é’ˆ (Instrumentation)...")
    # æ³¨å†Œ Hooks
    handles = []
    for name, module in model.named_modules():
        layer_type = None
        if isinstance(module, nn.Linear):
            layer_type = "Linear"
        elif "Norm" in module.__class__.__name__:
            layer_type = "Norm"
        elif "Act" in module.__class__.__name__ or "SiLU" in module.__class__.__name__:
            layer_type = "Activation"
            
        if layer_type:
            h1 = module.register_forward_pre_hook(pre_forward_hook)
            h2 = module.register_forward_hook(post_forward_hook(name, layer_type))
            handles.append(h1)
            handles.append(h2)

    # å‡†å¤‡è¾“å…¥
    input_text = "BitNet"
    inputs = tokenizer(input_text, return_tensors="pt")
    
    print("ğŸ”¥ é¢„çƒ­ (Warmup)...")
    with torch.no_grad():
        model(**inputs)
    
    # æ¸…ç©ºé¢„çƒ­æ•°æ®
    hooks_data.clear()
    pre_hook_cache.clear()
    
    print("ğŸƒ æ­£å¼æ¨ç†æµ‹è¯•...")
    with torch.no_grad():
        model(**inputs)
        
    # å¤„ç†æ•°æ®
    df = pd.DataFrame(hooks_data)
    
    # ç”Ÿæˆç»ˆææŠ¥å‘Š
    generate_full_report(df, model_str, load_mem)

if __name__ == "__main__":
    main()