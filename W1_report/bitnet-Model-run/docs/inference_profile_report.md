# BitNet b1.58 推理性能分析报告

**生成时间**: 2026-01-19 17:27:12

**模型路径**: `../models/microsoftbitnet-b1.58-2B-4T-bf16`

**数据类型**: `torch.float32`


## 1. 系统信息

### CPU 配置

| 属性 | 值 |
|------|-----|
| CPU 型号 | AMD Ryzen 7 9700X 8-Core Processor |
| 架构 | X86_64 |
| 物理核心数 | 8 |
| 逻辑核心数 | 16 |
| 基准频率 | 3793 MHz |
| 当前频率 | 3793 MHz |
| PyTorch 线程数 | 8 |

### 频率归一化

- **参考频率**: 3000 MHz
- **归一化系数**: 1.264
- **计算公式**: `归一化时间 = 实测时间 × (实际频率 / 参考频率)`

## 2. 推理阶段分析

| 阶段 | 耗时 (ms) | 归一化耗时 (ms) | 占比 (%) | 内存变化 (MB) |
|------|-----------|-----------------|----------|---------------|
| Tokenizer 加载 | 207.23 | 262.00 | 0.3 | +70.45 |
| Model 加载 | 2556.83 | 3232.50 | 3.2 | +9321.13 |
| Tokenization (编码) | 2.45 | 3.10 | 0.0 | +0.50 |
| Prefill (首次前向) | 4962.83 | 6274.31 | 6.2 | +59.58 |
| Decode (逐token生成) | 71942.02 | 90953.52 | 90.3 | +8.62 |
| Detokenization (解码) | 0.08 | 0.10 | 0.0 | +0.00 |
| **总计** | **79671.45** | **100725.53** | **100.0** | - |

### 汇总统计

- **总推理时间**: 79671.45 ms (79.67 s)
- **归一化总时间**: 100725.53 ms
- **峰值内存占用**: 9910.13 MB (9.68 GB)
- **生成 Token 数**: 50
- **平均每 Token 耗时**: 1438.43 ms
- **推理吞吐量**: 0.70 tokens/s

## 3. 部分层具体运行数据 (Raw Data)

**总计 Linear 层数量**: 211

**Linear 层总耗时**: 73073.31 ms


### Layer 0 (首层 - 包含初始化开销)

| Layer Name (层名) | Type (类型) | Input Shape (输入维度) | Time (ms) | Mem Delta (MB) | Abs Mem (MB) |
| --- | --- | --- | --- | --- | --- |
| **layers.0.mlp.down_proj** | **AutoBitLinear** | **[1, 1, 6912]** | **174.082** | **1.969** | **9910.2** |
| **layers.0.mlp.gate_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **51.700** | **0.375** | **9910.2** |
| **layers.0.mlp.up_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **13.490** | **-0.062** | **9910.1** |
| layers.0.self_attn.k_proj | AutoBitLinear | [1, 1, 2560] | 43.075 | 0.125 | 9910.1 |
| layers.0.self_attn.o_proj | AutoBitLinear | [1, 1, 2560] | 4.036 | 12.500 | 9910.1 |
| layers.0.self_attn.q_proj | AutoBitLinear | [1, 1, 2560] | 2918.632 | 38.000 | 9910.1 |
| layers.0.self_attn.v_proj | AutoBitLinear | [1, 1, 2560] | 0.695 | 0.000 | 9910.1 |
| layers.0 | BitNetDecoderLayer | - | 3205.711 | - | - |

> **数据解读**：第0层耗时显著高于后续层，这是由于包含了模型初始化、内存分配等一次性开销。

### Layer 1 (初始稳定运行阶段)

| Layer Name (层名) | Type (类型) | Input Shape (输入维度) | Time (ms) | Mem Delta (MB) | Abs Mem (MB) |
| --- | --- | --- | --- | --- | --- |
| **layers.1.mlp.down_proj** | **AutoBitLinear** | **[1, 1, 6912]** | **8.724** | **0.000** | **9910.1** |
| **layers.1.mlp.gate_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **10.152** | **-0.074** | **9910.1** |
| **layers.1.mlp.up_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **9.468** | **0.059** | **9910.3** |
| layers.1.self_attn.k_proj | AutoBitLinear | [1, 1, 2560] | 0.985 | 0.000 | 9910.2 |
| layers.1.self_attn.o_proj | AutoBitLinear | [1, 1, 2560] | 2.822 | 0.000 | 9910.2 |
| layers.1.self_attn.q_proj | AutoBitLinear | [1, 1, 2560] | 3.091 | 0.000 | 9910.2 |
| layers.1.self_attn.v_proj | AutoBitLinear | [1, 1, 2560] | 0.498 | 0.000 | 9910.2 |
| layers.1 | BitNetDecoderLayer | - | 35.740 | - | - |

> **数据解读**：第1层耗时降至正常水平，接近稳定运行状态。

### Layer 17 (中段稳定运行阶段)

| Layer Name (层名) | Type (类型) | Input Shape (输入维度) | Time (ms) | Mem Delta (MB) | Abs Mem (MB) |
| --- | --- | --- | --- | --- | --- |
| **layers.17.mlp.down_proj** | **AutoBitLinear** | **[1, 1, 6912]** | **11.21** | **-0.04** | **9910.1** |
| **layers.17.mlp.gate_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **13.41** | **-0.03** | **9910.2** |
| **layers.17.mlp.up_proj** | **AutoBitLinear** | **[1, 1, 2560]** | **12.13** | **0.04** | **9910.2** |
| layers.17.self_attn.k_proj | AutoBitLinear | [1, 1, 2560] | 0.92 | 0.00 | 9910.3 |
| layers.17.self_attn.o_proj | AutoBitLinear | [1, 1, 2560] | 2.88 | 0.00 | 9910.3 |
| layers.17.self_attn.q_proj | AutoBitLinear | [1, 1, 2560] | 3.15 | 0.00 | 9910.3 |
| layers.17.self_attn.v_proj | AutoBitLinear | [1, 1, 2560] | 0.65 | 0.00 | 9910.3 |
| layers.17 | BitNetDecoderLayer | - | 44.36 | - | - |

> **数据解读**：MLP 层的三个算子耗时显著高于 Attention 层。

---

## 4. 单层 (Decoder Layer) 平均耗时分布

| 模块类型 | 层数量 | 总耗时 (ms) | 占比 (%) | 总参数量 |
|----------|--------|-------------|----------|----------|
| `mlp.gate_proj` | 30 | 20212.09 | 27.66 | 530.84M |
| `mlp.up_proj` | 30 | 18321.19 | 25.07 | 530.84M |
| `mlp.down_proj` | 30 | 17857.63 | 24.44 | 530.84M |
| `self_attn.q_proj` | 30 | 7808.68 | 10.69 | 196.61M |
| `self_attn.o_proj` | 30 | 4420.79 | 6.05 | 196.61M |
| `lm_head` | 1 | 2024.02 | 2.77 | 328.34M |
| `self_attn.k_proj` | 30 | 1443.62 | 1.98 | 49.15M |
| `self_attn.v_proj` | 30 | 985.29 | 1.35 | 49.15M |

---

## 5. 算子级性能分析 (Operator-Level Analysis)

模型稳定运行阶段的每种算子核心数据统计与分析如下：

### 5.1 核心数据统计表

| 算子名称 (Submodule) | 平均耗时 (ms) | 时间占比 (%) | 单次运算量 (OPs) | 算力占比 (%) | 有效算力 (GOPS) | 内存增量 (MB) |
| --- | --- | --- | --- | --- | --- | --- |
| **gate_proj (MLP)** | **13.16** | **29.81%** | **35.39 M** | **25.47%** | **2.69** | **0.50** |
| **up_proj (MLP)** | **11.93** | **27.02%** | **35.39 M** | **25.47%** | **2.97** | **0.43** |
| **down_proj (MLP)** | **11.50** | **26.06%** | **35.39 M** | **25.47%** | **3.08** | **0.43** |
| q_proj (Attn) | 3.14 | 7.11% | 13.11 M | 9.43% | 4.18 | ~0 |
| o_proj (Attn) | 2.86 | 6.48% | 13.11 M | 9.43% | 4.58 | ~0 |
| k_proj (Attn) | 0.91 | 2.07% | 3.28 M | 2.36% | 3.59 | ~0 |
| v_proj (Attn) | 0.64 | 1.45% | 3.28 M | 2.36% | 5.11 | ~0 |

### 5.2 深度分析

#### 5.2.1 算力瓶颈分析 (Compute Bound)

**核心发现**：MLP 三算子（gate_proj、up_proj、down_proj）成为计算瓶颈。

**数据支撑**：

- MLP 三算子合计运算量：**3078.88M OPs**
- Attention 四算子合计运算量：**950.27M OPs**
- MLP 运算量占比：**76.4%**
- MLP 耗时占比：**82.9%**

**各算子运算效率对比**：

| 算子类型 | 单次运算量 | 平均耗时 | 运算效率 (OPs/ms) |
| --- | --- | --- | --- |
| gate_proj | 35.39M | 13.16ms | **2.69 GOPS** |
| up_proj | 35.39M | 11.93ms | **2.97 GOPS** |
| down_proj | 35.39M | 11.50ms | **3.08 GOPS** |
| q_proj | 13.11M | 3.14ms | **4.18 GOPS** |
| o_proj | 13.11M | 2.86ms | **4.58 GOPS** |
| k_proj | 3.28M | 0.91ms | **3.59 GOPS** |
| v_proj | 3.28M | 0.64ms | **5.11 GOPS** |

---

### 附录：完整 Linear 层列表

<details>
<summary>点击展开完整列表 (共 211 层)</summary>

| 层名称 | 调用次数 | 总耗时 (ms) | 占比 (%) | 输入形状 | 输出形状 | 参数量 |
|--------|----------|-------------|----------|----------|----------|--------|
| `model.layers.0.self_attn.q_proj` | 51 | 3122.19 | 4.27 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `lm_head` | 51 | 2024.02 | 2.77 | [1, 1, 2560] | [1, 1, 128256] | 328.34M |
| `model.layers.0.mlp.down_proj` | 51 | 758.68 | 1.04 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.0.mlp.gate_proj` | 51 | 710.01 | 0.97 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.18.mlp.gate_proj` | 51 | 692.44 | 0.95 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.9.mlp.gate_proj` | 51 | 684.52 | 0.94 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.17.mlp.gate_proj` | 51 | 683.98 | 0.94 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.20.mlp.gate_proj` | 51 | 683.08 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.22.mlp.gate_proj` | 51 | 681.25 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.8.mlp.gate_proj` | 51 | 681.23 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.26.mlp.gate_proj` | 51 | 680.88 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.5.mlp.gate_proj` | 51 | 680.87 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.13.mlp.gate_proj` | 51 | 679.25 | 0.93 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.16.mlp.gate_proj` | 51 | 675.08 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.1.mlp.gate_proj` | 51 | 675.02 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.7.mlp.gate_proj` | 51 | 674.80 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.6.mlp.gate_proj` | 51 | 672.84 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.25.mlp.gate_proj` | 51 | 672.76 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.24.mlp.gate_proj` | 51 | 671.89 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.28.mlp.gate_proj` | 51 | 671.79 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.4.mlp.gate_proj` | 51 | 671.55 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.29.mlp.gate_proj` | 51 | 671.37 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.11.mlp.gate_proj` | 51 | 669.40 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.12.mlp.gate_proj` | 51 | 669.10 | 0.92 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.27.mlp.gate_proj` | 51 | 667.19 | 0.91 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.15.mlp.gate_proj` | 51 | 666.02 | 0.91 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.14.mlp.gate_proj` | 51 | 665.49 | 0.91 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.3.mlp.gate_proj` | 51 | 664.40 | 0.91 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.21.mlp.gate_proj` | 51 | 662.84 | 0.91 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.10.mlp.gate_proj` | 51 | 660.48 | 0.90 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.19.mlp.gate_proj` | 51 | 658.82 | 0.90 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.2.mlp.gate_proj` | 51 | 658.75 | 0.90 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.23.mlp.gate_proj` | 51 | 654.97 | 0.90 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.9.mlp.up_proj` | 51 | 643.87 | 0.88 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.8.mlp.down_proj` | 51 | 635.60 | 0.87 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.6.mlp.up_proj` | 51 | 633.62 | 0.87 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.8.mlp.up_proj` | 51 | 632.42 | 0.87 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.4.mlp.up_proj` | 51 | 631.98 | 0.86 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.7.mlp.up_proj` | 51 | 628.50 | 0.86 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.5.mlp.up_proj` | 51 | 627.38 | 0.86 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.17.mlp.up_proj` | 51 | 618.76 | 0.85 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.18.mlp.up_proj` | 51 | 618.09 | 0.85 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.26.mlp.up_proj` | 51 | 616.68 | 0.84 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.11.mlp.up_proj` | 51 | 612.42 | 0.84 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.28.mlp.up_proj` | 51 | 611.66 | 0.84 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.25.mlp.up_proj` | 51 | 611.58 | 0.84 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.23.mlp.up_proj` | 51 | 611.41 | 0.84 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.5.mlp.down_proj` | 51 | 611.00 | 0.84 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.4.mlp.down_proj` | 51 | 610.06 | 0.83 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.12.mlp.up_proj` | 51 | 609.37 | 0.83 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.0.mlp.up_proj` | 51 | 608.37 | 0.83 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.9.mlp.down_proj` | 51 | 607.37 | 0.83 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.21.mlp.up_proj` | 51 | 607.06 | 0.83 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.15.mlp.up_proj` | 51 | 605.76 | 0.83 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.16.mlp.up_proj` | 51 | 604.76 | 0.83 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.6.mlp.down_proj` | 51 | 602.95 | 0.83 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.20.mlp.up_proj` | 51 | 602.56 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.1.mlp.up_proj` | 51 | 601.94 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.22.mlp.up_proj` | 51 | 601.74 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.18.mlp.down_proj` | 51 | 601.40 | 0.82 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.13.mlp.up_proj` | 51 | 601.39 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.10.mlp.up_proj` | 51 | 600.84 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.29.mlp.up_proj` | 51 | 599.98 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.27.mlp.up_proj` | 51 | 599.94 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.2.mlp.up_proj` | 51 | 599.84 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.24.mlp.up_proj` | 51 | 597.42 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.20.mlp.down_proj` | 51 | 596.52 | 0.82 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.19.mlp.up_proj` | 51 | 595.89 | 0.82 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.3.mlp.up_proj` | 51 | 594.09 | 0.81 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.1.mlp.down_proj` | 51 | 592.51 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.14.mlp.up_proj` | 51 | 591.85 | 0.81 | [1, 1, 2560] | [1, 1, 6912] | 17.69M |
| `model.layers.7.mlp.down_proj` | 51 | 591.75 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.27.mlp.down_proj` | 51 | 591.47 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.23.mlp.down_proj` | 51 | 591.34 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.13.mlp.down_proj` | 51 | 590.26 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.25.mlp.down_proj` | 51 | 589.78 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.16.mlp.down_proj` | 51 | 588.84 | 0.81 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.22.mlp.down_proj` | 51 | 585.52 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.21.mlp.down_proj` | 51 | 584.78 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.29.mlp.down_proj` | 51 | 584.65 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.10.mlp.down_proj` | 51 | 584.58 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.14.mlp.down_proj` | 51 | 584.15 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.28.mlp.down_proj` | 51 | 583.15 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.11.mlp.down_proj` | 51 | 582.22 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.3.mlp.down_proj` | 51 | 581.56 | 0.80 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.26.mlp.down_proj` | 51 | 579.97 | 0.79 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.24.mlp.down_proj` | 51 | 579.40 | 0.79 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.12.mlp.down_proj` | 51 | 577.48 | 0.79 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.15.mlp.down_proj` | 51 | 577.35 | 0.79 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.19.mlp.down_proj` | 51 | 575.43 | 0.79 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.17.mlp.down_proj` | 51 | 571.89 | 0.78 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.2.mlp.down_proj` | 51 | 565.96 | 0.77 | [1, 1, 6912] | [1, 1, 2560] | 17.69M |
| `model.layers.14.self_attn.q_proj` | 51 | 172.33 | 0.24 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.27.self_attn.q_proj` | 51 | 164.62 | 0.23 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.18.self_attn.q_proj` | 51 | 164.04 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.5.self_attn.q_proj` | 51 | 163.95 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.29.self_attn.q_proj` | 51 | 163.52 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.4.self_attn.q_proj` | 51 | 163.34 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.25.self_attn.q_proj` | 51 | 162.81 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.9.self_attn.q_proj` | 51 | 162.72 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.12.self_attn.q_proj` | 51 | 162.71 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.28.self_attn.q_proj` | 51 | 162.57 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.8.self_attn.q_proj` | 51 | 161.35 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.20.self_attn.q_proj` | 51 | 161.25 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.19.self_attn.q_proj` | 51 | 161.25 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.26.self_attn.q_proj` | 51 | 160.98 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.13.self_attn.q_proj` | 51 | 160.90 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.24.self_attn.q_proj` | 51 | 160.80 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.17.self_attn.q_proj` | 51 | 160.78 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.10.self_attn.q_proj` | 51 | 160.42 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.3.self_attn.q_proj` | 51 | 160.36 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.21.self_attn.q_proj` | 51 | 160.06 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.16.self_attn.q_proj` | 51 | 160.02 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.23.self_attn.q_proj` | 51 | 160.02 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.6.self_attn.q_proj` | 51 | 159.79 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.22.self_attn.q_proj` | 51 | 159.68 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.7.self_attn.q_proj` | 51 | 159.66 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.15.self_attn.q_proj` | 51 | 159.61 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.11.self_attn.q_proj` | 51 | 159.59 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.2.self_attn.q_proj` | 51 | 159.28 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.1.self_attn.q_proj` | 51 | 158.09 | 0.22 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.9.self_attn.o_proj` | 51 | 154.84 | 0.21 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.5.self_attn.o_proj` | 51 | 151.56 | 0.21 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.8.self_attn.o_proj` | 51 | 151.31 | 0.21 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.4.self_attn.o_proj` | 51 | 151.31 | 0.21 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.7.self_attn.o_proj` | 51 | 150.33 | 0.21 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.28.self_attn.o_proj` | 51 | 149.61 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.0.self_attn.o_proj` | 51 | 149.34 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.12.self_attn.o_proj` | 51 | 148.57 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.22.self_attn.o_proj` | 51 | 148.56 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.6.self_attn.o_proj` | 51 | 148.40 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.14.self_attn.o_proj` | 51 | 148.23 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.1.self_attn.o_proj` | 51 | 147.54 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.19.self_attn.o_proj` | 51 | 147.33 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.2.self_attn.o_proj` | 51 | 147.23 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.26.self_attn.o_proj` | 51 | 147.20 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.27.self_attn.o_proj` | 51 | 147.16 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.29.self_attn.o_proj` | 51 | 147.13 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.15.self_attn.o_proj` | 51 | 146.98 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.13.self_attn.o_proj` | 51 | 146.74 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.17.self_attn.o_proj` | 51 | 146.71 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.11.self_attn.o_proj` | 51 | 146.52 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.10.self_attn.o_proj` | 51 | 146.42 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.3.self_attn.o_proj` | 51 | 146.36 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.20.self_attn.o_proj` | 51 | 146.33 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.21.self_attn.o_proj` | 51 | 145.70 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.16.self_attn.o_proj` | 51 | 145.13 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.23.self_attn.o_proj` | 51 | 144.79 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.18.self_attn.o_proj` | 51 | 144.28 | 0.20 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.24.self_attn.o_proj` | 51 | 140.80 | 0.19 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.25.self_attn.o_proj` | 51 | 138.37 | 0.19 | [1, 1, 2560] | [1, 1, 2560] | 6.55M |
| `model.layers.0.self_attn.k_proj` | 51 | 84.81 | 0.12 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.18.self_attn.k_proj` | 51 | 48.75 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.16.self_attn.k_proj` | 51 | 48.65 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.27.self_attn.k_proj` | 51 | 48.26 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.5.self_attn.k_proj` | 51 | 47.94 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.12.self_attn.k_proj` | 51 | 47.84 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.9.self_attn.k_proj` | 51 | 47.72 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.20.self_attn.k_proj` | 51 | 47.69 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.19.self_attn.k_proj` | 51 | 47.67 | 0.07 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.14.self_attn.k_proj` | 51 | 47.43 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.4.self_attn.k_proj` | 51 | 47.41 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.8.self_attn.k_proj` | 51 | 47.23 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.13.self_attn.k_proj` | 51 | 47.22 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.17.self_attn.k_proj` | 51 | 47.16 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.26.self_attn.k_proj` | 51 | 46.92 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.15.self_attn.k_proj` | 51 | 46.81 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.22.self_attn.k_proj` | 51 | 46.81 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.11.self_attn.k_proj` | 51 | 46.56 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.28.self_attn.k_proj` | 51 | 46.45 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.29.self_attn.k_proj` | 51 | 46.43 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.6.self_attn.k_proj` | 51 | 46.18 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.3.self_attn.k_proj` | 51 | 46.00 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.2.self_attn.k_proj` | 51 | 45.98 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.10.self_attn.k_proj` | 51 | 45.98 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.21.self_attn.k_proj` | 51 | 45.93 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.24.self_attn.k_proj` | 51 | 45.87 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.7.self_attn.k_proj` | 51 | 45.78 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.1.self_attn.k_proj` | 51 | 45.73 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.23.self_attn.k_proj` | 51 | 45.60 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.25.self_attn.k_proj` | 51 | 44.81 | 0.06 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.11.self_attn.v_proj` | 51 | 35.33 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.8.self_attn.v_proj` | 51 | 34.33 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.5.self_attn.v_proj` | 51 | 34.15 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.27.self_attn.v_proj` | 51 | 33.96 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.9.self_attn.v_proj` | 51 | 33.90 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.13.self_attn.v_proj` | 51 | 33.54 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.19.self_attn.v_proj` | 51 | 33.53 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.12.self_attn.v_proj` | 51 | 33.53 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.4.self_attn.v_proj` | 51 | 33.52 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.14.self_attn.v_proj` | 51 | 33.35 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.16.self_attn.v_proj` | 51 | 33.16 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.17.self_attn.v_proj` | 51 | 33.13 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.22.self_attn.v_proj` | 51 | 33.05 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.7.self_attn.v_proj` | 51 | 33.04 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.28.self_attn.v_proj` | 51 | 32.95 | 0.05 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.3.self_attn.v_proj` | 51 | 32.79 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.10.self_attn.v_proj` | 51 | 32.75 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.15.self_attn.v_proj` | 51 | 32.68 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.24.self_attn.v_proj` | 51 | 32.60 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.20.self_attn.v_proj` | 51 | 32.36 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.21.self_attn.v_proj` | 51 | 32.34 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.6.self_attn.v_proj` | 51 | 32.30 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.18.self_attn.v_proj` | 51 | 32.09 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.23.self_attn.v_proj` | 51 | 31.99 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.1.self_attn.v_proj` | 51 | 31.84 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.2.self_attn.v_proj` | 51 | 31.77 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.29.self_attn.v_proj` | 51 | 31.60 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.25.self_attn.v_proj` | 51 | 31.52 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.0.self_attn.v_proj` | 51 | 31.09 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |
| `model.layers.26.self_attn.v_proj` | 51 | 31.09 | 0.04 | [1, 1, 2560] | [1, 1, 640] | 1.64M |

</details>

## 6. Token 生成详情

### 6.1 统计信息

- **生成 Token 数**: 50
- **最小耗时**: 1169.61 ms
- **最大耗时**: 1563.09 ms
- **平均耗时**: 1438.43 ms
- **标准差**: 115.43 ms
- **中位数**: 1479.23 ms

### 6.2 Token 生成列表

<details>
<summary>点击展开完整列表 (共 50 个 Token)</summary>

| 序号 | Token ID | Token 文本 | 耗时 (ms) | 归一化耗时 (ms) | 内存 (MB) |
|------|----------|------------|-----------|-----------------|-----------|
| 1 | 264 | ` a` | 1300.96 | 1644.75 | 9901.50 |
| 2 | 3728 | ` global` | 1188.71 | 1502.84 | 9902.88 |
| 3 | 5557 | ` technology` | 1174.39 | 1484.74 | 9902.81 |
| 4 | 2883 | ` company` | 1169.61 | 1478.70 | 9902.98 |
| 5 | 430 | ` that` | 1184.16 | 1497.09 | 9903.05 |
| 6 | 19159 | ` produces` | 1170.23 | 1479.48 | 9903.08 |
| 7 | 264 | ` a` | 1177.41 | 1488.55 | 9903.49 |
| 8 | 2134 | ` range` | 1174.96 | 1485.45 | 9903.84 |
| 9 | 315 | ` of` | 1308.33 | 1654.07 | 9903.78 |
| 10 | 12035 | ` hardware` | 1517.61 | 1918.65 | 9903.80 |
| 11 | 323 | ` and` | 1434.96 | 1814.17 | 9903.93 |
| 12 | 3241 | ` software` | 1428.77 | 1806.34 | 9903.87 |
| 13 | 3956 | ` products` | 1486.59 | 1879.43 | 9903.85 |
| 14 | 11 | `,` | 1509.65 | 1908.59 | 9904.40 |
| 15 | 2737 | ` including` | 1547.88 | 1956.93 | 9904.88 |
| 16 | 10565 | ` operating` | 1500.01 | 1896.40 | 9904.64 |
| 17 | 6067 | ` systems` | 1488.81 | 1882.25 | 9904.82 |
| 18 | 11 | `,` | 1441.95 | 1823.00 | 9904.68 |
| 19 | 5274 | ` office` | 1549.15 | 1958.53 | 9904.84 |
| 20 | 8522 | ` applications` | 1563.09 | 1976.15 | 9905.15 |
| 21 | 11 | `,` | 1542.39 | 1949.99 | 9904.89 |
| 22 | 323 | ` and` | 1544.48 | 1952.63 | 9905.14 |
| 23 | 11761 | ` consumer` | 1484.55 | 1876.86 | 9905.74 |
| 24 | 31591 | ` electronics` | 1462.64 | 1849.15 | 9905.96 |
| 25 | 382 | `.\n\n` | 1483.50 | 1875.53 | 9906.10 |
| 26 | 13068 | `Microsoft` | 1456.85 | 1841.84 | 9906.30 |
| 27 | 596 | `'s` | 1455.67 | 1840.35 | 9906.75 |
| 28 | 10565 | ` operating` | 1476.92 | 1867.21 | 9907.04 |
| 29 | 1887 | ` system` | 1465.28 | 1852.49 | 9906.96 |
| 30 | 11 | `,` | 1462.11 | 1848.50 | 9907.30 |
| 31 | 5632 | ` Windows` | 1460.32 | 1846.22 | 9907.82 |
| 32 | 52128 | `')]` | 1513.67 | 1913.68 | 9907.80 |
| 33 | 374 | ` is` | 1516.50 | 1917.25 | 9908.01 |
| 34 | 264 | ` a` | 1455.32 | 1839.90 | 9908.21 |
| 35 | 5526 | ` popular` | 1459.85 | 1845.63 | 9908.53 |
| 36 | 3241 | ` software` | 1481.54 | 1873.06 | 9908.81 |
| 37 | 430 | ` that` | 1482.73 | 1874.56 | 9909.18 |
| 38 | 374 | ` is` | 1456.81 | 1841.79 | 9909.23 |
| 39 | 1511 | ` used` | 1494.69 | 1889.68 | 9909.85 |
| 40 | 555 | ` by` | 1468.89 | 1857.06 | 9910.24 |
| 41 | 11990 | ` millions` | 1516.84 | 1917.68 | 9910.17 |
| 42 | 315 | ` of` | 1498.14 | 1894.04 | 9910.16 |
| 43 | 1274 | ` people` | 1463.67 | 1850.46 | 9910.19 |
| 44 | 2212 | ` around` | 1485.90 | 1878.57 | 9910.17 |
| 45 | 279 | ` the` | 1486.17 | 1878.90 | 9910.12 |
| 46 | 1917 | ` world` | 1497.11 | 1892.74 | 9910.02 |
| 47 | 13 | `.` | 1499.77 | 1896.10 | 9910.00 |
| 48 | 1102 | ` It` | 1472.84 | 1862.05 | 9910.03 |
| 49 | 374 | ` is` | 1511.49 | 1910.91 | 9910.08 |
| 50 | 3967 | ` known` | 1547.70 | 1956.70 | 9910.13 |

</details>

## 7. 内存占用时间线

| 时间点 (s) | 进程 RSS (MB) | 虚拟内存 (MB) | 系统已用 (MB) | 系统总量 (MB) |
|------------|---------------|---------------|---------------|---------------|
| 0.00 | 449.60 | 4374.68 | 7345.97 | 30912.73 |
| 0.21 | 520.04 | 4450.90 | 7412.67 | 30912.73 |
| 0.21 | 520.04 | 4450.90 | 7412.67 | 30912.73 |
| 2.76 | 9841.18 | 14544.50 | 17010.09 | 30912.73 |
| 2.77 | 9841.30 | 14544.50 | 17010.34 | 30912.73 |
| 2.77 | 9841.80 | 15600.57 | 17010.34 | 30912.73 |
| 2.77 | 9841.80 | 15600.57 | 17010.34 | 30912.73 |
| 7.73 | 9901.38 | 15719.27 | 17055.30 | 30912.73 |
| 79.68 | 9910.06 | 15720.32 | 16969.05 | 30912.73 |
| 79.68 | 9910.06 | 15720.32 | 16969.05 | 30912.73 |

## 8. FPGA 加速建议

### 加速目标层

根据性能分析，建议优先将以下层卸载到 FPGA 加速：

- **Top 5 层占比**: 10.0% 的 Linear 层计算时间

1. `model.layers.0.self_attn.q_proj`
   - 耗时占比: 4.27%
   - 参数量: 6,553,600
   - 输入形状: [1, 1, 2560]
   - 输出形状: [1, 1, 2560]

2. `lm_head`
   - 耗时占比: 2.77%
   - 参数量: 328,335,360
   - 输入形状: [1, 1, 2560]
   - 输出形状: [1, 1, 128256]

3. `model.layers.0.mlp.down_proj`
   - 耗时占比: 1.04%
   - 参数量: 17,694,720
   - 输入形状: [1, 1, 6912]
   - 输出形状: [1, 1, 2560]

4. `model.layers.0.mlp.gate_proj`
   - 耗时占比: 0.97%
   - 参数量: 17,694,720
   - 输入形状: [1, 1, 2560]
   - 输出形状: [1, 1, 6912]

5. `model.layers.18.mlp.gate_proj`
   - 耗时占比: 0.95%
   - 参数量: 17,694,720
   - 输入形状: [1, 1, 2560]
   - 输出形状: [1, 1, 6912]

### 1.58-bit 量化收益预估

| 指标 | FP32 | 1.58-bit | 压缩比 |
|------|------|----------|--------|
| Linear 层权重大小 | 9202.50 MB | 575.16 MB | 16.0x |

---

*本报告由 BitNet 推理性能分析工具自动生成*