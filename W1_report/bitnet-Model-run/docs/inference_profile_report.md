# BitNet b1.58 推理性能分析报告

**生成时间**: 2026-01-19 16:41:06

**模型路径**: `../models/microsoftbitnet-b1.58-2B-4T-bf16`

**数据类型**: `torch.float32`


## 1. 系统信息

### CPU 配置

| 属性 | 值 |
|------|-----|
| CPU 型号 | AMD Ryzen 7 9700X 8-Core Processor |
| 架构 | X86_64 |
| 物理核心数 | 8 |
| 逻辑核心数 | 16 |
| 基准频率 | 3793 MHz |
| 当前频率 | 3793 MHz |
| PyTorch 线程数 | 8 |

### 频率归一化

- **参考频率**: 3000 MHz
- **归一化系数**: 1.264
- **计算公式**: `归一化时间 = 实测时间 × (实际频率 / 参考频率)`

## 2. 推理阶段分析

| 阶段 | 耗时 (ms) | 归一化耗时 (ms) | 占比 (%) | 内存变化 (MB) |
|------|-----------|-----------------|----------|---------------|
| Tokenizer 加载 | 238.48 | 301.51 | 0.3 | +70.50 |
| Model 加载 | 4127.54 | 5218.29 | 5.3 | +9320.31 |
| Tokenization (编码) | 2.77 | 3.51 | 0.0 | +0.62 |
| Prefill (首次前向) | 7758.78 | 9809.13 | 9.9 | +54.10 |
| Decode (逐token生成) | 66137.80 | 83615.47 | 84.5 | +8.92 |
| Detokenization (解码) | 0.08 | 0.10 | 0.0 | +0.00 |
| **总计** | **78265.47** | **98948.00** | **100.0** | - |

### 汇总统计

- **总推理时间**: 78265.47 ms (78.27 s)
- **归一化总时间**: 98948.00 ms
- **峰值内存占用**: 9901.12 MB (9.67 GB)
- **生成 Token 数**: 50
- **平均每 Token 耗时**: 1322.28 ms
- **推理吞吐量**: 0.76 tokens/s

## 3. 线性层 (Linear Layer) 详细分析

**总计 Linear 层数量**: 211

**Linear 层总耗时**: 69729.61 ms


### 3.1 最耗时的 Linear 层 (Top 20)

| 排名 | 层名称 | 调用次数 | 总耗时 (ms) | 归一化耗时 (ms) | 占比 (%) | 参数量 |
|------|--------|----------|-------------|-----------------|----------|--------|
| 1 | `model.layers.0.self_attn.q_proj` | 51 | 4969.12 | 6282.26 | 7.13 | 6.55M |
| 2 | `lm_head` | 51 | 2208.01 | 2791.51 | 3.17 | 328.34M |
| 3 | `model.layers.0.mlp.down_proj` | 51 | 1806.24 | 2283.55 | 2.59 | 17.69M |
| 4 | `model.layers.0.mlp.gate_proj` | 51 | 620.85 | 784.91 | 0.89 | 17.69M |
| 5 | `model.layers.0.mlp.up_proj` | 51 | 575.28 | 727.30 | 0.83 | 17.69M |
| 6 | `model.layers.11.mlp.gate_proj` | 51 | 574.76 | 726.64 | 0.82 | 17.69M |
| 7 | `model.layers.16.mlp.gate_proj` | 51 | 569.52 | 720.03 | 0.82 | 17.69M |
| 8 | `model.layers.17.mlp.gate_proj` | 51 | 567.49 | 717.46 | 0.81 | 17.69M |
| 9 | `model.layers.9.mlp.gate_proj` | 51 | 566.21 | 715.84 | 0.81 | 17.69M |
| 10 | `model.layers.1.mlp.gate_proj` | 51 | 565.95 | 715.51 | 0.81 | 17.69M |
| 11 | `model.layers.22.mlp.gate_proj` | 51 | 565.21 | 714.58 | 0.81 | 17.69M |
| 12 | `model.layers.7.mlp.gate_proj` | 51 | 564.35 | 713.49 | 0.81 | 17.69M |
| 13 | `model.layers.16.mlp.up_proj` | 51 | 561.73 | 710.17 | 0.81 | 17.69M |
| 14 | `model.layers.8.mlp.gate_proj` | 51 | 561.70 | 710.14 | 0.81 | 17.69M |
| 15 | `model.layers.27.mlp.up_proj` | 51 | 561.62 | 710.04 | 0.81 | 17.69M |
| 16 | `model.layers.6.mlp.gate_proj` | 51 | 560.61 | 708.76 | 0.80 | 17.69M |
| 17 | `model.layers.18.mlp.gate_proj` | 51 | 560.10 | 708.11 | 0.80 | 17.69M |
| 18 | `model.layers.26.mlp.gate_proj` | 51 | 560.03 | 708.03 | 0.80 | 17.69M |
| 19 | `model.layers.6.mlp.up_proj` | 51 | 559.72 | 707.63 | 0.80 | 17.69M |
| 20 | `model.layers.2.mlp.gate_proj` | 51 | 559.66 | 707.56 | 0.80 | 17.69M |

### 3.2 按模块分组统计

| 模块类型 | 层数量 | 总耗时 (ms) | 占比 (%) | 总参数量 |
|----------|--------|-------------|----------|----------|
| `mlp.down_proj` | 30 | 17270.58 | 24.77 | 530.84M |
| `mlp.gate_proj` | 30 | 16846.60 | 24.16 | 530.84M |
| `mlp.up_proj` | 30 | 16459.40 | 23.60 | 530.84M |
| `self_attn.q_proj` | 30 | 10114.36 | 14.51 | 196.61M |
| `self_attn.o_proj` | 30 | 4506.48 | 6.46 | 196.61M |
| `lm_head` | 1 | 2208.01 | 3.17 | 328.34M |
| `self_attn.k_proj` | 30 | 1397.08 | 2.00 | 49.15M |
| `self_attn.v_proj` | 30 | 927.10 | 1.33 | 49.15M |

### 3.3 完整 Linear 层列表

<details>
<summary>点击展开完整列表 (共 211 层)</summary>

| 层名称 | 调用次数 | 总耗时 (ms) | 占比 (%) | 输入形状 | 输出形状 | 参数量 |
|--------|----------|-------------|----------|----------|----------|--------|
| `model.layers.0.self_attn.q_proj` | 51 | 4969.12 | 7.13 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `lm_head` | 51 | 2208.01 | 3.17 | (1, 1, 2560) | (1, 1, 128256) | 328.34M |
| `model.layers.0.mlp.down_proj` | 51 | 1806.24 | 2.59 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.0.mlp.gate_proj` | 51 | 620.85 | 0.89 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.0.mlp.up_proj` | 51 | 575.28 | 0.83 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.11.mlp.gate_proj` | 51 | 574.76 | 0.82 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.16.mlp.gate_proj` | 51 | 569.52 | 0.82 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.17.mlp.gate_proj` | 51 | 567.49 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.9.mlp.gate_proj` | 51 | 566.21 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.1.mlp.gate_proj` | 51 | 565.95 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.22.mlp.gate_proj` | 51 | 565.21 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.7.mlp.gate_proj` | 51 | 564.35 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.16.mlp.up_proj` | 51 | 561.73 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.8.mlp.gate_proj` | 51 | 561.70 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.27.mlp.up_proj` | 51 | 561.62 | 0.81 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.6.mlp.gate_proj` | 51 | 560.61 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.18.mlp.gate_proj` | 51 | 560.10 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.26.mlp.gate_proj` | 51 | 560.03 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.6.mlp.up_proj` | 51 | 559.72 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.2.mlp.gate_proj` | 51 | 559.66 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.20.mlp.gate_proj` | 51 | 559.38 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.25.mlp.gate_proj` | 51 | 559.11 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.23.mlp.gate_proj` | 51 | 558.76 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.19.mlp.gate_proj` | 51 | 558.75 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.28.mlp.gate_proj` | 51 | 557.68 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.5.mlp.gate_proj` | 51 | 557.37 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.29.mlp.gate_proj` | 51 | 557.24 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.10.mlp.gate_proj` | 51 | 557.23 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.14.mlp.gate_proj` | 51 | 555.89 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.27.mlp.gate_proj` | 51 | 555.53 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.17.mlp.up_proj` | 51 | 555.20 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.13.mlp.gate_proj` | 51 | 555.03 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.3.mlp.gate_proj` | 51 | 554.95 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.1.mlp.up_proj` | 51 | 554.90 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.21.mlp.gate_proj` | 51 | 554.83 | 0.80 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.12.mlp.gate_proj` | 51 | 553.61 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.24.mlp.gate_proj` | 51 | 553.55 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.15.mlp.gate_proj` | 51 | 552.69 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.7.mlp.up_proj` | 51 | 552.39 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.13.mlp.up_proj` | 51 | 551.99 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.26.mlp.up_proj` | 51 | 551.42 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.18.mlp.up_proj` | 51 | 549.54 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.28.mlp.up_proj` | 51 | 549.29 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.9.mlp.up_proj` | 51 | 549.05 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.4.mlp.gate_proj` | 51 | 548.56 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.29.mlp.up_proj` | 51 | 548.29 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.2.mlp.up_proj` | 51 | 547.45 | 0.79 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.6.mlp.down_proj` | 51 | 547.21 | 0.78 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.20.mlp.up_proj` | 51 | 547.13 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.24.mlp.up_proj` | 51 | 547.03 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.23.mlp.up_proj` | 51 | 547.00 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.12.mlp.up_proj` | 51 | 546.13 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.1.mlp.down_proj` | 51 | 546.04 | 0.78 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.19.mlp.up_proj` | 51 | 545.53 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.7.mlp.down_proj` | 51 | 545.12 | 0.78 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.8.mlp.up_proj` | 51 | 545.01 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.17.mlp.down_proj` | 51 | 544.90 | 0.78 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.15.mlp.up_proj` | 51 | 544.37 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.11.mlp.up_proj` | 51 | 544.37 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.16.mlp.down_proj` | 51 | 544.10 | 0.78 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.5.mlp.up_proj` | 51 | 543.10 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.4.mlp.up_proj` | 51 | 542.65 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.25.mlp.up_proj` | 51 | 542.28 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.14.mlp.up_proj` | 51 | 541.42 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.10.mlp.up_proj` | 51 | 540.53 | 0.78 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.10.mlp.down_proj` | 51 | 539.99 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.22.mlp.up_proj` | 51 | 539.70 | 0.77 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.8.mlp.down_proj` | 51 | 539.32 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.3.mlp.up_proj` | 51 | 538.12 | 0.77 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.21.mlp.up_proj` | 51 | 537.16 | 0.77 | (1, 1, 2560) | (1, 1, 6912) | 17.69M |
| `model.layers.27.mlp.down_proj` | 51 | 536.71 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.26.mlp.down_proj` | 51 | 536.18 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.2.mlp.down_proj` | 51 | 534.83 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.25.mlp.down_proj` | 51 | 534.44 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.24.mlp.down_proj` | 51 | 533.74 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.23.mlp.down_proj` | 51 | 533.47 | 0.77 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.13.mlp.down_proj` | 51 | 533.11 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.12.mlp.down_proj` | 51 | 531.56 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.4.mlp.down_proj` | 51 | 530.51 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.29.mlp.down_proj` | 51 | 530.21 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.11.mlp.down_proj` | 51 | 529.88 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.18.mlp.down_proj` | 51 | 529.47 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.15.mlp.down_proj` | 51 | 529.25 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.22.mlp.down_proj` | 51 | 528.82 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.14.mlp.down_proj` | 51 | 528.26 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.9.mlp.down_proj` | 51 | 528.16 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.19.mlp.down_proj` | 51 | 527.79 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.28.mlp.down_proj` | 51 | 527.51 | 0.76 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.21.mlp.down_proj` | 51 | 526.39 | 0.75 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.5.mlp.down_proj` | 51 | 525.14 | 0.75 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.20.mlp.down_proj` | 51 | 523.68 | 0.75 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.3.mlp.down_proj` | 51 | 518.57 | 0.74 | (1, 1, 6912) | (1, 1, 2560) | 17.69M |
| `model.layers.28.self_attn.q_proj` | 51 | 181.42 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.7.self_attn.q_proj` | 51 | 181.10 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.24.self_attn.q_proj` | 51 | 179.47 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.11.self_attn.q_proj` | 51 | 179.28 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.2.self_attn.q_proj` | 51 | 178.92 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.21.self_attn.q_proj` | 51 | 178.88 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.18.self_attn.q_proj` | 51 | 178.80 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.13.self_attn.q_proj` | 51 | 178.79 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.25.self_attn.q_proj` | 51 | 178.68 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.27.self_attn.q_proj` | 51 | 178.52 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.17.self_attn.q_proj` | 51 | 177.85 | 0.26 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.8.self_attn.q_proj` | 51 | 177.76 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.29.self_attn.q_proj` | 51 | 177.65 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.23.self_attn.q_proj` | 51 | 177.65 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.16.self_attn.q_proj` | 51 | 177.57 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.3.self_attn.q_proj` | 51 | 177.40 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.5.self_attn.q_proj` | 51 | 177.37 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.20.self_attn.q_proj` | 51 | 177.28 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.22.self_attn.q_proj` | 51 | 177.01 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.26.self_attn.q_proj` | 51 | 176.65 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.1.self_attn.q_proj` | 51 | 176.57 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.15.self_attn.q_proj` | 51 | 176.26 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.14.self_attn.q_proj` | 51 | 175.99 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.19.self_attn.q_proj` | 51 | 175.55 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.10.self_attn.q_proj` | 51 | 175.41 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.9.self_attn.q_proj` | 51 | 175.13 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.6.self_attn.q_proj` | 51 | 174.78 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.12.self_attn.q_proj` | 51 | 174.62 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.4.self_attn.q_proj` | 51 | 172.87 | 0.25 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.0.self_attn.o_proj` | 51 | 167.05 | 0.24 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.7.self_attn.o_proj` | 51 | 155.16 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.17.self_attn.o_proj` | 51 | 153.88 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.9.self_attn.o_proj` | 51 | 152.13 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.12.self_attn.o_proj` | 51 | 152.03 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.1.self_attn.o_proj` | 51 | 152.02 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.28.self_attn.o_proj` | 51 | 151.78 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.15.self_attn.o_proj` | 51 | 151.34 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.16.self_attn.o_proj` | 51 | 151.01 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.26.self_attn.o_proj` | 51 | 150.94 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.25.self_attn.o_proj` | 51 | 150.94 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.5.self_attn.o_proj` | 51 | 150.92 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.6.self_attn.o_proj` | 51 | 150.49 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.24.self_attn.o_proj` | 51 | 150.45 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.4.self_attn.o_proj` | 51 | 150.22 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.3.self_attn.o_proj` | 51 | 149.96 | 0.22 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.27.self_attn.o_proj` | 51 | 149.57 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.20.self_attn.o_proj` | 51 | 149.29 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.18.self_attn.o_proj` | 51 | 148.83 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.10.self_attn.o_proj` | 51 | 148.75 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.11.self_attn.o_proj` | 51 | 148.70 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.2.self_attn.o_proj` | 51 | 148.67 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.29.self_attn.o_proj` | 51 | 148.19 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.14.self_attn.o_proj` | 51 | 147.62 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.13.self_attn.o_proj` | 51 | 147.42 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.19.self_attn.o_proj` | 51 | 147.09 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.8.self_attn.o_proj` | 51 | 146.30 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.23.self_attn.o_proj` | 51 | 145.80 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.22.self_attn.o_proj` | 51 | 145.77 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.21.self_attn.o_proj` | 51 | 144.15 | 0.21 | (1, 1, 2560) | (1, 1, 2560) | 6.55M |
| `model.layers.0.self_attn.k_proj` | 51 | 75.22 | 0.11 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.7.self_attn.k_proj` | 51 | 48.25 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.5.self_attn.k_proj` | 51 | 46.76 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.17.self_attn.k_proj` | 51 | 46.71 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.14.self_attn.k_proj` | 51 | 46.30 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.28.self_attn.k_proj` | 51 | 46.22 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.11.self_attn.k_proj` | 51 | 46.19 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.18.self_attn.k_proj` | 51 | 46.06 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.26.self_attn.k_proj` | 51 | 46.02 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.29.self_attn.k_proj` | 51 | 45.93 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.6.self_attn.k_proj` | 51 | 45.86 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.3.self_attn.k_proj` | 51 | 45.86 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.27.self_attn.k_proj` | 51 | 45.82 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.15.self_attn.k_proj` | 51 | 45.76 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.20.self_attn.k_proj` | 51 | 45.73 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.4.self_attn.k_proj` | 51 | 45.69 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.25.self_attn.k_proj` | 51 | 45.60 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.8.self_attn.k_proj` | 51 | 45.56 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.2.self_attn.k_proj` | 51 | 45.54 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.21.self_attn.k_proj` | 51 | 45.50 | 0.07 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.13.self_attn.k_proj` | 51 | 45.19 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.9.self_attn.k_proj` | 51 | 45.07 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.1.self_attn.k_proj` | 51 | 44.97 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.22.self_attn.k_proj` | 51 | 44.88 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.10.self_attn.k_proj` | 51 | 44.66 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.24.self_attn.k_proj` | 51 | 44.54 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.19.self_attn.k_proj` | 51 | 44.49 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.16.self_attn.k_proj` | 51 | 44.46 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.12.self_attn.k_proj` | 51 | 44.35 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.23.self_attn.k_proj` | 51 | 43.90 | 0.06 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.25.self_attn.v_proj` | 51 | 32.64 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.18.self_attn.v_proj` | 51 | 32.38 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.11.self_attn.v_proj` | 51 | 32.35 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.28.self_attn.v_proj` | 51 | 31.95 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.15.self_attn.v_proj` | 51 | 31.87 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.2.self_attn.v_proj` | 51 | 31.75 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.27.self_attn.v_proj` | 51 | 31.54 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.29.self_attn.v_proj` | 51 | 31.51 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.7.self_attn.v_proj` | 51 | 31.51 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.26.self_attn.v_proj` | 51 | 31.47 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.5.self_attn.v_proj` | 51 | 31.44 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.17.self_attn.v_proj` | 51 | 31.42 | 0.05 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.13.self_attn.v_proj` | 51 | 31.00 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.19.self_attn.v_proj` | 51 | 30.88 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.1.self_attn.v_proj` | 51 | 30.82 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.20.self_attn.v_proj` | 51 | 30.79 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.3.self_attn.v_proj` | 51 | 30.76 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.6.self_attn.v_proj` | 51 | 30.76 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.21.self_attn.v_proj` | 51 | 30.72 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.4.self_attn.v_proj` | 51 | 30.61 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.8.self_attn.v_proj` | 51 | 30.60 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.16.self_attn.v_proj` | 51 | 30.49 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.23.self_attn.v_proj` | 51 | 30.45 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.9.self_attn.v_proj` | 51 | 30.08 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.14.self_attn.v_proj` | 51 | 30.03 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.22.self_attn.v_proj` | 51 | 30.00 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.10.self_attn.v_proj` | 51 | 29.99 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.12.self_attn.v_proj` | 51 | 29.57 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.24.self_attn.v_proj` | 51 | 29.41 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |
| `model.layers.0.self_attn.v_proj` | 51 | 28.34 | 0.04 | (1, 1, 2560) | (1, 1, 640) | 1.64M |

</details>

## 4. Token 生成详情

### 4.1 统计信息

- **生成 Token 数**: 50
- **最小耗时**: 1278.12 ms
- **最大耗时**: 1437.21 ms
- **平均耗时**: 1322.28 ms
- **标准差**: 31.81 ms
- **中位数**: 1316.09 ms

### 4.2 Token 生成列表

<details>
<summary>点击展开完整列表 (共 50 个 Token)</summary>

| 序号 | Token ID | Token 文本 | 耗时 (ms) | 归一化耗时 (ms) | 内存 (MB) |
|------|----------|------------|-----------|-----------------|-----------|
| 1 | 8173 | ` interested` | 1416.69 | 1791.06 | 9891.61 |
| 2 | 304 | ` in` | 1367.08 | 1728.35 | 9893.24 |
| 3 | 19546 | ` obtaining` | 1390.41 | 1757.84 | 9893.88 |
| 4 | 682 | ` all` | 1372.12 | 1734.72 | 9893.84 |
| 5 | 279 | ` the` | 1336.33 | 1689.47 | 9893.94 |
| 6 | 3649 | ` details` | 1327.69 | 1678.55 | 9893.84 |
| 7 | 315 | ` of` | 1288.85 | 1629.44 | 9894.35 |
| 8 | 1148 | ` what` | 1278.12 | 1615.88 | 9894.19 |
| 9 | 753 | `’s` | 1285.88 | 1625.68 | 9894.74 |
| 10 | 12765 | ` happening` | 1326.97 | 1677.63 | 9894.93 |
| 11 | 304 | ` in` | 1329.48 | 1680.81 | 9894.88 |
| 12 | 279 | ` the` | 1331.97 | 1683.96 | 9894.55 |
| 13 | 3728 | ` global` | 1312.00 | 1658.71 | 9894.85 |
| 14 | 3157 | ` market` | 1351.67 | 1708.87 | 9894.91 |
| 15 | 11 | `,` | 1437.21 | 1817.00 | 9894.87 |
| 16 | 779 | ` so` | 1354.92 | 1712.97 | 9894.88 |
| 17 | 433 | ` it` | 1308.13 | 1653.82 | 9895.43 |
| 18 | 649 | ` can` | 1294.51 | 1636.60 | 9895.52 |
| 19 | 8417 | ` determine` | 1311.85 | 1658.53 | 9895.46 |
| 20 | 279 | ` the` | 1299.61 | 1643.05 | 9895.98 |
| 21 | 1888 | ` best` | 1310.13 | 1656.35 | 9896.02 |
| 22 | 3388 | ` course` | 1286.03 | 1625.88 | 9896.12 |
| 23 | 315 | ` of` | 1291.24 | 1632.47 | 9896.11 |
| 24 | 1957 | ` action` | 1294.72 | 1636.87 | 9896.42 |
| 25 | 369 | ` for` | 1304.00 | 1648.59 | 9896.32 |
| 26 | 279 | ` the` | 1296.87 | 1639.58 | 9896.77 |
| 27 | 2883 | ` company` | 1314.48 | 1661.84 | 9897.12 |
| 28 | 13 | `.` | 1290.72 | 1631.81 | 9897.37 |
| 29 | 1102 | ` It` | 1317.95 | 1666.23 | 9897.72 |
| 30 | 1587 | ` does` | 1316.20 | 1664.02 | 9897.61 |
| 31 | 539 | ` not` | 1290.59 | 1631.64 | 9897.77 |
| 32 | 1390 | ` want` | 1315.36 | 1662.95 | 9898.30 |
| 33 | 311 | ` to` | 1302.25 | 1646.39 | 9898.25 |
| 34 | 9229 | ` lose` | 1295.41 | 1637.73 | 9899.54 |
| 35 | 704 | ` out` | 1304.29 | 1648.96 | 9899.73 |
| 36 | 389 | ` on` | 1312.44 | 1659.27 | 9899.51 |
| 37 | 904 | ` any` | 1319.22 | 1667.84 | 9899.59 |
| 38 | 4754 | ` potential` | 1335.63 | 1688.58 | 9899.36 |
| 39 | 13254 | ` revenue` | 1320.01 | 1668.84 | 9899.64 |
| 40 | 323 | ` and` | 1331.99 | 1683.98 | 9899.48 |
| 41 | 433 | ` it` | 1299.49 | 1642.89 | 9899.54 |
| 42 | 6944 | ` wants` | 1323.50 | 1673.26 | 9899.53 |
| 43 | 311 | ` to` | 1328.21 | 1679.21 | 9899.68 |
| 44 | 6106 | ` ensure` | 1316.10 | 1663.90 | 9899.86 |
| 45 | 430 | ` that` | 1319.66 | 1668.40 | 9900.00 |
| 46 | 433 | ` it` | 1327.06 | 1677.76 | 9900.40 |
| 47 | 753 | `’s` | 1337.96 | 1691.53 | 9900.50 |
| 48 | 8405 | ` providing` | 1364.01 | 1724.47 | 9900.53 |
| 49 | 279 | ` the` | 1310.89 | 1657.31 | 9900.63 |
| 50 | 1888 | ` best` | 1316.08 | 1663.87 | 9900.54 |

</details>

## 5. 内存占用时间线

| 时间点 (s) | 进程 RSS (MB) | 虚拟内存 (MB) | 系统已用 (MB) | 系统总量 (MB) |
|------------|---------------|---------------|---------------|---------------|
| 0.00 | 445.95 | 4374.08 | 6501.12 | 30912.73 |
| 0.24 | 516.45 | 4450.34 | 6569.36 | 30912.73 |
| 0.24 | 516.45 | 4450.34 | 6569.36 | 30912.73 |
| 4.37 | 9836.77 | 14542.95 | 16172.73 | 30912.73 |
| 4.37 | 9836.89 | 14542.95 | 16172.73 | 30912.73 |
| 4.37 | 9837.52 | 15599.01 | 16170.73 | 30912.73 |
| 4.37 | 9837.52 | 15599.01 | 16164.69 | 30912.73 |
| 12.13 | 9891.61 | 15785.22 | 16290.67 | 30912.73 |
| 78.27 | 9901.12 | 15786.85 | 16226.46 | 30912.73 |
| 78.27 | 9901.12 | 15786.85 | 16226.46 | 30912.73 |

## 6. FPGA 加速建议

### 加速目标层

根据性能分析，建议优先将以下层卸载到 FPGA 加速：

- **Top 5 层占比**: 14.6% 的 Linear 层计算时间

1. `model.layers.0.self_attn.q_proj`
   - 耗时占比: 7.13%
   - 参数量: 6,553,600
   - 输入形状: (1, 1, 2560)
   - 输出形状: (1, 1, 2560)

2. `lm_head`
   - 耗时占比: 3.17%
   - 参数量: 328,335,360
   - 输入形状: (1, 1, 2560)
   - 输出形状: (1, 1, 128256)

3. `model.layers.0.mlp.down_proj`
   - 耗时占比: 2.59%
   - 参数量: 17,694,720
   - 输入形状: (1, 1, 6912)
   - 输出形状: (1, 1, 2560)

4. `model.layers.0.mlp.gate_proj`
   - 耗时占比: 0.89%
   - 参数量: 17,694,720
   - 输入形状: (1, 1, 2560)
   - 输出形状: (1, 1, 6912)

5. `model.layers.0.mlp.up_proj`
   - 耗时占比: 0.83%
   - 参数量: 17,694,720
   - 输入形状: (1, 1, 2560)
   - 输出形状: (1, 1, 6912)

### 1.58-bit 量化收益预估

| 指标 | FP32 | 1.58-bit | 压缩比 |
|------|------|----------|--------|
| Linear 层权重大小 | 9202.50 MB | 575.16 MB | 16.0x |

---

*本报告由 BitNet 推理性能分析工具自动生成*