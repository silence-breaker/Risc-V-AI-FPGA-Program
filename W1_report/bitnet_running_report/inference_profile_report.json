[
    {
        "Layer Name": "model.layers.0.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5750139998781378,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.0.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 19.991047000075923,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.0.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 7.4351690000185044,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.0.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.330297000024075,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.0.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.8609310000338155,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.0.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 15.38997400007247,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.0.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6186059999890858,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.16015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.0.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 40.26278500009539,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.0.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.19952399998146575,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.0.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 41.24383399994258,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.0.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4171160001078533,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.0.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 42.232266000155505,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.1.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6989820001308544,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.1.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 16.785634999905596,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.1.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.693631999998615,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.1.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.447474000016882,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.1.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6082100001094659,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.1.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 11.685457000112365,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.1.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.290260000156195,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.1.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 34.42393499994978,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.6484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.1.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.15961999997671228,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6484375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.1.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 36.25028199985536,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.1.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.6415459999971063,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.1.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 26.410746000010477,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.2.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5431350000435486,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.2.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 11.53045299997757,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.2.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.663325000137775,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.2.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.527445000097941,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.2.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.8201380001082725,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.2.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 10.435302000132651,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.2.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.36838099981650885,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.2.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 40.20156499996119,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.2.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.28708000013466517,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.2.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 41.04016899987073,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.0078125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.2.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 1.6139330000441987,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0078125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.2.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 37.59797099996831,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.3.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.7811709999714367,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.3.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.734364999891113,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.3.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.987531000073432,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.3.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 7.478405000028943,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.3.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.9460580001814378,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.3.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 16.792438999800652,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.3.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6085360000724904,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.62890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.3.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 45.13287500003571,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.3.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2746769998793752,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.3.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 33.250756000143156,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.3.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.8555919998798345,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.3.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 39.211797999996634,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.4.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3421219998926972,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.4.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 18.665388000044913,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.4.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.2679420000695245,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.4.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.591197999843644,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.4.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6459349999659025,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.4.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 10.04400900001201,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.4.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3851030000987521,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.4.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 28.85806600011165,
        "Mem Delta (MB)": 0.09375,
        "Abs Mem (MB)": 5385.9921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.4.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.17169400007333024,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9921875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.4.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 32.5002949998634,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.61328125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.4.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.40145999992091674,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.61328125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.4.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 33.40490300001875,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.5.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.5817839998817362,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.5.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.567495999912353,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.5.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.399540999907913,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.5.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.0528659999854426,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.5.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3269410001394135,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.5.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 8.293097999967358,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.5.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6354059999011952,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.5.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 30.517906999875777,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.5.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.12983399983568233,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.5.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 33.224422999865055,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.8828125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.5.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.8732829999189562,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8828125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.5.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 31.783383999936632,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.6.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.27973100009148766,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.6.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 8.046021999916775,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.6.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.3442950000571727,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.6.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.7567659999476746,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.6.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3092040001320129,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.6.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 8.454471000050034,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.6.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4984089998743002,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.6.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 29.848145999949338,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.75,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.6.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.1748150000366877,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.75,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.6.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 24.760965999803375,
        "Mem Delta (MB)": 0.24609375,
        "Abs Mem (MB)": 5385.99609375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.6.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4380540001420741,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.99609375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.6.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 31.229448999965825,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.7.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6598939999094,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.7.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 15.084908000062569,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.7.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.391698000063116,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.7.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.249407000088468,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.7.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32327700000678306,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.7.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.741193000010753,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.7.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4990009999801259,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9296875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.7.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 29.6379110000089,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.86328125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.7.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.43858200001523073,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.86328125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.7.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 24.777541000048586,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.953125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.7.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4568940000808652,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.953125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.7.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 20.558244000085324,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.8.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.37588899999718706,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.8.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 8.120073000100092,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.8.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.0512950002048456,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.8.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.259409000065716,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.8.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.23941399990690115,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.8.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 7.691778999969756,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.8.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2579859999514156,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.73046875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.8.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 18.46360499985167,
        "Mem Delta (MB)": 0.24609375,
        "Abs Mem (MB)": 5385.9765625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.8.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.15531300005022786,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9765625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.8.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 21.594631999960257,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.91015625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.8.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.3723999998328509,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91015625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.8.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 22.17800200014608,
        "Mem Delta (MB)": 0.09375,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.9.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.43862699999408505,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.9.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.9360760001254675,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.9.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.2358480000548298,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.9.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.250628999898254,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.9.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2535509997869667,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.9.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 8.10104499987574,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.9.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2277159999266587,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.9.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 21.312049000016486,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.9.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.1337769999736338,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.9.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 21.312966999857963,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.9.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.47443499988730764,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.9.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 26.96337800011861,
        "Mem Delta (MB)": 0.24609375,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.10.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.376860999949713,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.10.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 10.376761000088663,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.10.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.573161000053005,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.10.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.901285999949323,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.10.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3661949999695935,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.10.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.075544999992417,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.10.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5798370000320574,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.10.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 39.19937000000573,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.10.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.3783059999022953,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.10.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 37.81091499990907,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.984375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.10.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.9711799998513015,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.984375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.10.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 33.90352199994595,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.11.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4573119999804476,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.11.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 11.530475999961709,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.11.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.607767999914358,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.11.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.00451499990595,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.11.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.2003609999737819,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.11.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 9.763845999941623,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.11.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.24875700000848155,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.91796875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.11.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 18.325202999903922,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.6953125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.11.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2930949999608856,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6953125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.11.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.695077999931527,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.09765625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.11.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4146559999753663,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.09765625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.11.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 21.538425000017014,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.12.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.41506100001242885,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.12.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.192785999975968,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.12.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.4421989999154903,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.12.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.5666369999962626,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.12.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2592529999674298,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.12.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.011165999827426,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.12.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.8844930000577733,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.12.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.86100100009935,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.12.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.33462000010331394,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.12.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.461127999789824,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.12.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.36931899990122474,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8984375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.12.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.215363999961482,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.13.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.45341299983192584,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.13.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.172524000068734,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.13.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.671357999839529,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.13.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.5398460000142222,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.13.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.23062500008563802,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.13.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.5927760000049602,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.13.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4654940000818897,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.67578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.13.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.452708999944662,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.078125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.13.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.20107799991819775,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.078125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.13.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.819758000181537,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5386.01171875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.13.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.38840399997752684,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.01171875,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.13.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 14.536067000108233,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.14.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4243270000188204,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.14.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.0461369999320596,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.14.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.4390999999704945,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.14.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.2687060000189376,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.14.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5001259999062313,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.14.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.1690340001423465,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.14.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2592880000520381,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9453125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.14.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.266081999996459,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.03515625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.14.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.13191799985179387,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.03515625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.14.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.690759999917645,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.65625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.14.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.540668999974514,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.65625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.14.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 14.077375000169923,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.15.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3901489999407204,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.15.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.8091250000888977,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.15.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.4232769999343873,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.15.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.1921789998723398,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.15.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.234844000033263,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.15.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.343022000080964,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.15.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2465200000187906,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05859375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.15.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.743478999936997,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.15.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.17897900011121237,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9921875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.15.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.060084000017014,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.92578125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.15.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.3766499999073858,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.92578125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.15.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.00459599997339,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.16.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4054839998843818,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.16.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.68728800003737,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.16.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.4183709999997518,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.16.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.0215519998837408,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.16.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4136430000016844,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.16.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.392598000094949,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.16.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5813440000110859,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.16.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.644948000068325,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.63671875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.16.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2231570001640648,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.63671875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.16.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.927391999937754,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.16.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.334091000013359,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.16.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.04362900009437,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.17.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.8008430002064415,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.17.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.882302999931198,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.17.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.258516999972926,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.17.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.3564650000716938,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.17.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32482999995409045,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.17.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.8971479998508585,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.17.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.28295199990679976,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.12890625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.17.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.010223000013866,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.17.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.18325099995308847,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.17.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.81843500009927,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.99609375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.17.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2694739998787554,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.99609375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.17.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.221485000030953,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.18.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3495930000099179,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.18.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.700087999959578,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.18.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.4734300000327494,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.18.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.2423379998836026,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.18.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2721140001540334,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.18.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.097364000017478,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.18.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.26375599986749876,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.6171875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.18.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.02895300000273,
        "Mem Delta (MB)": 0.40234375,
        "Abs Mem (MB)": 5386.01953125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.18.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2189679998991778,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.01953125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.18.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.199640000038926,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.109375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.18.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.5726730000787938,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.109375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.18.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 17.560578000029636,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.19.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4219649999868125,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.19.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.7145069999933185,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.19.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.7738720001907495,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.19.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.7230949999648146,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.19.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.23516700002801372,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.19.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.42181500013794,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.19.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.27880599986929155,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.88671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.19.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.979778000044462,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.9765625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.19.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2064739999241283,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9765625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.19.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 15.033101999961218,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.75390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.19.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.5369419998260128,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.75390625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.19.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 15.052743000069313,
        "Mem Delta (MB)": 0.24609375,
        "Abs Mem (MB)": 5386.0,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.20.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.0544219999246707,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.20.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 6.084301999862873,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.20.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.889355000002979,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.20.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.1254220000391797,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.20.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3018810000412486,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.20.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.513656999961313,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.20.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2959190001092793,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.20.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 16.03274500007501,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.08984375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.20.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2074639999136707,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.08984375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.20.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.693492999867885,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.8671875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.20.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.5672680001680419,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.8671875,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.20.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 15.531790000068213,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.21.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4435620001004281,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.21.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 6.704041000148209,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.21.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.6425460000609746,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.21.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.7214550000517193,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.21.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32777199999145523,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.21.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.04758300010144,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.21.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2707090000058088,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.95703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.21.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.851693000082378,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.734375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.21.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2323000001069886,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.734375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.21.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.87147700006608,
        "Mem Delta (MB)": 0.24609375,
        "Abs Mem (MB)": 5385.98046875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.21.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4352280000148312,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98046875,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.21.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 17.012774000022546,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.22.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6393099999968399,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.22.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.821177000143507,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.22.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.8422529999497783,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.22.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.7520729998068418,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.22.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3824899999926856,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.22.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.794823999873188,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.22.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4039200000534038,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0703125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.22.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.567801000112013,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.22.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.5029200001445133,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.00390625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.22.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.643004999925324,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.22.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 1.2299980000989308,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9375,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.22.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 16.70012699992185,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.23.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.4352260000123351,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.23.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.623618000072383,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.23.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.9481839999571093,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.23.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.058760000181792,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.23.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.4250450001563877,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.23.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.5804970000062895,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.23.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32217400007539254,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.71484375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.23.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 15.403810999941925,
        "Mem Delta (MB)": 0.25,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.23.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.14639100004387728,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.96484375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.23.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.845549000052415,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.0546875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.23.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.4450159999578318,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0546875,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.23.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.726935000022422,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.24.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2615950002109457,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.24.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.3479089999891585,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.24.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.7409590000170283,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.24.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.5039330000709015,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.24.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.5090629999813245,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.24.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.793842000050063,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.24.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3098610000051849,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.98828125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.24.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 15.957012999933795,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.24.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.26934199991046626,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.921875,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.24.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.754844999970373,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5385.69921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.24.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.46124800019242684,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.69921875,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.24.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 15.118774000029589,
        "Mem Delta (MB)": 0.55859375,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.25.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.6798770000386867,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.25.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.328048000161289,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.25.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.5475720001631998,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.25.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.577464000092732,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.25.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.39325800003098266,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.25.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.21395900002608,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.25.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3702929998325999,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.2578125,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.25.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 12.411351000082504,
        "Mem Delta (MB)": -0.21875,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.25.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.22672599993711628,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.25.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.503590000003896,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.25.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.36798399992221675,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.97265625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.25.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 13.902443000006315,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.26.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.35526600004232023,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.26.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.430264999882638,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.26.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.0141100001183077,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.26.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.9035440000152448,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.26.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.3303989999494661,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.26.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.188257000123485,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.26.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.45064699997965363,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.90625,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.26.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.912392999827716,
        "Mem Delta (MB)": -0.21484375,
        "Abs Mem (MB)": 5385.69140625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.26.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.2675260000160051,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.69140625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.26.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.836625000043568,
        "Mem Delta (MB)": 0.55859375,
        "Abs Mem (MB)": 5386.25,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.26.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.42259099996044824,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.25,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.26.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 15.543016000037824,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.27.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.017221999973117,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.27.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.00777799993557,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.27.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.609513999937917,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.27.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.1182119999375573,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.27.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32868700009203167,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.27.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 4.5157949998611,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.27.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.49797100018622587,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.02734375,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.27.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.699707000090712,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.27.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.23139699987950735,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.9609375,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.27.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 14.20956400011164,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.27.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.5363930001749395,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.05078125,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.27.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 16.019681999978275,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.28.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.529561999883299,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.28.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 5.429491999848324,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.28.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.3105210000503575,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.28.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.0708009999689239,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.28.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2290899999479734,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.28.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.506779999952414,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.28.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.2368400000705151,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.671875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.28.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.652428000114014,
        "Mem Delta (MB)": 0.56640625,
        "Abs Mem (MB)": 5386.23828125,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.28.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.16629299989290303,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.23828125,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.28.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.643876000060118,
        "Mem Delta (MB)": -0.22265625,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.28.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.301461999924868,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.015625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.28.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 10.565528999904927,
        "Mem Delta (MB)": -0.06640625,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.layers.29.input_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.30274899995674787,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "input_layernorm"
    },
    {
        "Layer Name": "model.layers.29.self_attn.q_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 2.771634000055201,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "q_proj"
    },
    {
        "Layer Name": "model.layers.29.self_attn.k_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.6486020001593715,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "k_proj"
    },
    {
        "Layer Name": "model.layers.29.self_attn.v_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 1.3300690000050963,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 9830400,
        "Category": "Attention",
        "ShortName": "v_proj"
    },
    {
        "Layer Name": "model.layers.29.self_attn.attn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.48289100004694774,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "attn_sub_norm"
    },
    {
        "Layer Name": "model.layers.29.self_attn.o_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 3.845099000045593,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 39321600,
        "Category": "Attention",
        "ShortName": "o_proj"
    },
    {
        "Layer Name": "model.layers.29.post_attention_layernorm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.32360599993808137,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.94921875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "post_attention_layernorm"
    },
    {
        "Layer Name": "model.layers.29.mlp.gate_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 13.982900999963022,
        "Mem Delta (MB)": 0.08984375,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "gate_proj"
    },
    {
        "Layer Name": "model.layers.29.mlp.act_fn",
        "Type": "Activation",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.37878899979659764,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.0390625,
        "OPs": 41472,
        "Category": "Activation",
        "ShortName": "act_fn"
    },
    {
        "Layer Name": "model.layers.29.mlp.up_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 11.929721000115023,
        "Mem Delta (MB)": -0.37890625,
        "Abs Mem (MB)": 5385.66015625,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "up_proj"
    },
    {
        "Layer Name": "model.layers.29.mlp.ffn_sub_norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 0.3409619998819835,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5385.66015625,
        "OPs": 62208,
        "Category": "Norm",
        "ShortName": "ffn_sub_norm"
    },
    {
        "Layer Name": "model.layers.29.mlp.down_proj",
        "Type": "Linear",
        "Input Shape": "[1, 3, 6912]",
        "Time (ms)": 12.607285999820306,
        "Mem Delta (MB)": 0.55859375,
        "Abs Mem (MB)": 5386.21875,
        "OPs": 106168320,
        "Category": "MLP",
        "ShortName": "down_proj"
    },
    {
        "Layer Name": "model.norm",
        "Type": "Norm",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 0.36439299992707674,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.21875,
        "OPs": 23040,
        "Category": "Norm",
        "ShortName": "norm"
    },
    {
        "Layer Name": "lm_head",
        "Type": "Linear",
        "Input Shape": "[1, 3, 2560]",
        "Time (ms)": 37.912990999984686,
        "Mem Delta (MB)": 0.0,
        "Abs Mem (MB)": 5386.21875,
        "OPs": 1970012160,
        "Category": "Other",
        "ShortName": "lm_head"
    }
]